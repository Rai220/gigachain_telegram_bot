{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Evaluation в GigaLogger\n",
    "В этом ноутбуке мы произведем оценку нашего RAG'а с помощью датасета и мощной LLM (gpt-4o)\n",
    "И не только! Мы также замерим качество ответов на обычном GigaChat (без RAG), с обычным RAG, и Adaptive RAG.\n",
    "У нас в боте используется Adaptive RAG.\n",
    "Предыдущие шаги:\n",
    "1. [Генерация синтетического датасета](generate_dataset.ipynb)\n",
    "2. [Загрузка датасета в GigaLogger](gigalogger_create_dataset.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import getpass\n",
    "\n",
    "def get_env_var(var_name):\n",
    "    if var_name in os.environ:\n",
    "        return os.environ[var_name]\n",
    "    else:\n",
    "        return getpass.getpass(f\"Enter {var_name}: \")\n",
    "\n",
    "load_dotenv(find_dotenv())\n",
    "os.environ[\"LANGFUSE_HOST\"] = \"https://gigalogger.demo.sberdevices.ru\"\n",
    "os.environ[\"LANGFUSE_PUBLIC_KEY\"] = get_env_var(\"LANGFUSE_PUBLIC_KEY\")\n",
    "os.environ[\"LANGFUSE_SECRET_KEY\"] = get_env_var(\"LANGFUSE_SECRET_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from langfuse import Langfuse\n",
    "langfuse = Langfuse()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Цепочка для ответов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Определим промпт для цепочки, которая будет сравнивать наши ответы с правильными ответами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "COT_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"query\", \"context\", \"result\"], template=\"\"\"Ты учитель, оценивающий тест.\n",
    "Тебе дан вопрос, контекст, к которому относится вопрос, и ответ студента. Тебе нужно оценить ответ студента как ПРАВИЛЬНЫЙ или НЕПРАВИЛЬНЫЙ, основываясь на контексте.\n",
    "Опиши пошагово своё рассуждение, чтобы убедиться, что твой вывод правильный. Избегай просто указывать правильный ответ с самого начала.\n",
    "\n",
    "Пример формата:\n",
    "QUESTION: здесь вопрос\n",
    "CONTEXT: здесь контекст, к которому относится вопрос\n",
    "STUDENT ANSWER: здесь ответ студента\n",
    "EXPLANATION: пошаговое рассуждение здесь\n",
    "GRADE: CORRECT или INCORRECT здесь\n",
    "\n",
    "Оценивай ответы студента ТОЛЬКО на основе их фактической точности. Игнорируй различия в пунктуации и формулировках между ответом студента и правильным ответом. Ответ студента может содержать больше информации, чем правильный ответ, если в нём нет противоречивых утверждений. Начнём!\n",
    "\n",
    "QUESTION: \"{query}\"\n",
    "CONTEXT: \"{context}\"\n",
    "STUDENT ANSWER: \"{result}\"\n",
    "EXPLANATION:\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from langchain.evaluation import CotQAEvalChain\n",
    "from langchain_openai import ChatOpenAI\n",
    "def cot_llm(query, output, expected_output):\n",
    "    eval_llm = ChatOpenAI(temperature=0)\n",
    "    eval_chain = CotQAEvalChain.from_llm(llm=eval_llm, prompt=COT_PROMPT)\n",
    "    resp = eval_chain.invoke({\n",
    "        \"query\": query, \"context\": expected_output, \"result\": output\n",
    "    })\n",
    "    return eval_chain._prepare_output(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Проверим работу цепочки оценки ответов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'reasoning': 'Главный герой книги, контекст которой \"Собака\", скорее всего будет собакой, а не котом. Поэтому ответ студента \"Кот\" неверный.\\nGRADE: INCORRECT',\n",
       " 'value': 'INCORRECT',\n",
       " 'score': 0}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Тут оценка неправильного ответа от LLM\n",
    "cot_llm(\"Кто главный герой книги\", \"Кот\", \"Собака\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'reasoning': 'Студент ответил \"Кот\", что является правильным ответом, так как в контексте вопроса упоминается \"Котик\". Хотя ответ студента короче, чем вопрос, он все равно передает правильную информацию.\\nGRADE: CORRECT',\n",
       " 'value': 'CORRECT',\n",
       " 'score': 1}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Тут оценка правильного ответа от LLM\n",
    "cot_llm(\"Кто главный герой книги\", \"Кот\", \"Котик\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Оценка\n",
    "### Оценка ответов с обычным GigaChat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from langchain_community.chat_models import GigaChat\n",
    "llm = GigaChat(model=\"GigaChat-Pro\", profanity_check=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 16/35 [03:09<03:51, 12.19s/it]Giga generation stopped with reason: blacklist\n",
      "100%|██████████| 35/35 [06:51<00:00, 11.77s/it]\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "dataset = langfuse.get_dataset(\"rag_dataset\")\n",
    "\n",
    "for item in tqdm.tqdm(dataset.items):\n",
    "    handler = item.get_langchain_handler(run_name=\"llm_without_rag_2\")\n",
    "    try:\n",
    "        generation = llm.invoke(input=item.input, config={\"callbacks\": [handler]}).content\n",
    "        resp = cot_llm(item.input, generation, item.expected_output)\n",
    "        handler.trace.score(\n",
    "            name=\"cot_llm\",\n",
    "            value=resp['score'],\n",
    "            comment=resp['reasoning']\n",
    "        )\n",
    "    except Exception as e:\n",
    "        handler.trace.score(\n",
    "            name=\"cot_llm\",\n",
    "            value=0,\n",
    "            comment=str(e)\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Первый прогон сделан. Смотрим результат...\n",
    "![image.png](media/llm_without_rag_2.png)\n",
    "Результат вышел `0.42`.\n",
    "Судя по всему GigaChat хорошо справляется с вопросами сам о себе, но про GigaChain отвечает слабо.\n",
    "Теперь попробуем прогнать датасет с простым RAG\n",
    "### Оценка ответов GigaChat + RAG(стандартный)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from graph import vector_store\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(llm, retriever=vector_store.as_retriever())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'Как обновить GigaChain?',\n",
       " 'result': 'Для обновления GigaChain выполните команду `bash pip install -U gigachain_community`.'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_chain.invoke(\"Как обновить GigaChain?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35/35 [04:59<00:00,  8.56s/it]\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "dataset = langfuse.get_dataset(\"rag_dataset\")\n",
    "\n",
    "for item in tqdm.tqdm(dataset.items):\n",
    "    handler = item.get_langchain_handler(run_name=\"llm_with_rag_2\")\n",
    "    try:\n",
    "        generation = qa_chain.invoke(input=item.input, config={\"callbacks\": [handler]})['result']\n",
    "        resp = cot_llm(item.input, generation, item.expected_output)\n",
    "        handler.trace.score(\n",
    "            name=\"cot_llm\",\n",
    "            value=resp['score'],\n",
    "            comment=resp['reasoning']\n",
    "        )\n",
    "    except Exception as e:\n",
    "        handler.trace.score(\n",
    "            name=\"cot_llm\",\n",
    "            value=0,\n",
    "            comment=str(e)\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Смотрим результат...\n",
    "![image.png](media/llm_with_rag.png)\n",
    "Результат вышел `0.68`.\n",
    "### Оценка ответов GigaChat + Adaptive RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/19563044/Documents/giga/gigachain_telegram_bot/venv/lib/python3.12/site-packages/pinecone/data/index.py:1: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")  # Add the parent folder to the sys.path\n",
    "from graph import graph, GraphState"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Для обновления библиотеки GigaChain используйте следующую команду в терминале: `bash pip install -U gigachain_community`. Если у вас возникли вопросы по использованию этой команды или другие проблемы, пожалуйста, уточните ваш запрос.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.invoke(input=GraphState(question=\"Как обновить GigaChain?\"))['generation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35/35 [11:10<00:00, 19.15s/it]\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "import logging\n",
    "dataset = langfuse.get_dataset(\"rag_dataset\")\n",
    "\n",
    "for item in tqdm.tqdm(dataset.items):\n",
    "    handler = item.get_langchain_handler(run_name=\"llm_with_arag\")\n",
    "    try:\n",
    "        s = GraphState(question=item.input)\n",
    "        generation = graph.invoke(input=s, config={\"callbacks\": [handler]})['generation']\n",
    "        resp = cot_llm(item.input, generation, item.expected_output)\n",
    "        handler.trace.score(\n",
    "            name=\"cot_llm\",\n",
    "            value=resp['score'],\n",
    "            comment=resp['reasoning']\n",
    "        )\n",
    "    except Exception as e:\n",
    "        logging.error(e, exc_info=True)\n",
    "        handler.trace.score(\n",
    "            name=\"cot_llm\",\n",
    "            value=0,\n",
    "            comment=str(e)\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Смотрим результат...\n",
    "![image.png](media/llm_with_arag.png)\n",
    "Результат вышел `0.74`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
