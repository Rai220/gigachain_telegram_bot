{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Evaluation в GigaLogger\n",
    "В этом ноутбуке мы произведем оценку нашего RAG'а с помощью датасета и мощной LLM (gpt-4o)\n",
    "И не только! Мы также замерим качество ответов на обычном GigaChat (без RAG), с обычным RAG, и Adaptive RAG.\n",
    "У нас в боте используется Adaptive RAG.\n",
    "Предыдущие шаги:\n",
    "1. [Генерация синтетического датасета](1_generate_dataset.ipynb)\n",
    "2. [Загрузка датасета в GigaLogger](2_gigalogger_create_dataset.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import getpass\n",
    "\n",
    "def get_env_var(var_name):\n",
    "    if var_name in os.environ:\n",
    "        return os.environ[var_name]\n",
    "    else:\n",
    "        return getpass.getpass(f\"Enter {var_name}: \")\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")  # Add the parent folder to the sys.path\n",
    "\n",
    "load_dotenv(find_dotenv())\n",
    "os.environ[\"LANGFUSE_HOST\"] = \"https://gigalogger.demo.sberdevices.ru\"\n",
    "os.environ[\"LANGFUSE_PUBLIC_KEY\"] = get_env_var(\"LANGFUSE_PUBLIC_KEY\")\n",
    "os.environ[\"LANGFUSE_SECRET_KEY\"] = get_env_var(\"LANGFUSE_SECRET_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from langfuse import Langfuse\n",
    "langfuse = Langfuse()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Цепочка для оценки ответов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Определим промпты для оценки ответов\n",
    "Мы будем оценивать по следующим критериям:\n",
    "- Похожи ли ответ нашей цепочки и корректный ответ (из датасета)\n",
    "- Содержит ли ответ информацию из документов, которые мы нашли с помощью RAG\n",
    "- Есть ли в ответе ссылки из документов (или из стандартного раздела ссылок)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "COT_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"query\", \"context\", \"result\"], template=\"\"\"Ты учитель, оценивающий тест.\n",
    "\n",
    "Тебе дан вопрос, корректный ответ и ответ студента. Тебе нужно оценить ответ студента как ПРАВИЛЬНЫЙ или НЕПРАВИЛЬНЫЙ, основываясь на корректном ответе.\n",
    "Опиши пошагово своё рассуждение, чтобы убедиться, что твой вывод правильный. Избегай просто указывать правильный ответ с самого начала.\n",
    "\n",
    "Вот базовая информация из конкретной области этого теста:\n",
    "GigaChat - это большая языковая модель (LLM) от Сбера.\n",
    "GigaChat API (апи) - это API для взаимодействия с GigaChat по HTTP с помощью REST запросов.\n",
    "GigaChain - это SDK на Python для работы с GigaChat API. Русскоязычный форк библиотеки LangChain.\n",
    "GigaGraph - это дополнение для GigaChain, который позволяет создавать мультиагентные системы, описывая их в виде графов.\n",
    "Обучение GigaChat выполняется командой разработчиков. Дообучение и файнтюнинг для конечных пользователей на данный момент не доступно.\n",
    "Для получения доступа к API нужно зарегистрироваться на developers.sber.ru и получить авторизационные данные.\n",
    "\n",
    "Опирайся на эту базовую информацию, если тебе не хватает информации для проверки теста.\n",
    "\n",
    "Пример формата:\n",
    "QUESTION: здесь вопрос\n",
    "TRUE ANSWER: здесь корректный ответ\n",
    "STUDENT ANSWER: здесь ответ студента\n",
    "EXPLANATION: пошаговое рассуждение здесь\n",
    "GRADE: CORRECT или INCORRECT здесь\n",
    "\n",
    "Тебе будем дан только один ответ студента, не несколько.\n",
    "Оценивай ответ студента ТОЛЬКО на основе их фактической точности. Игнорируй различия в пунктуации и формулировках между ответом студента и правильным ответом. Ответ студента может содержать больше информации, чем правильный ответ, если в нём нет противоречивых утверждений, то он корректен. Начнём!\n",
    "\n",
    "QUESTION: \"{query}\"\n",
    "TRUE ANSWER: \"{context}\"\n",
    "STUDENT ANSWER: \"{result}\"\n",
    "EXPLANATION:\"\"\"\n",
    ")\n",
    "ANSWERED_ON_DOCUMENTS_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"answer\", \"documents\"], template=\"\"\"Ты учитель, оценивающий тест.\n",
    "Тебе будет дан ответ студента и документы, которые были даны студенту.\n",
    "Избегай просто указывать правильный ответ с самого начала.\n",
    "Ты должен оценить ответ студента исходя из следующих критериев:\n",
    "* Ответ студента основан на документах, которые были даны студенту\n",
    "* Ответ студента содержит ссылки из документов, относящихся к вопросу или ссылки из дополнительного блока ссылок\n",
    "\n",
    "Ответ студента: \"{answer}\"\n",
    "Документы: \"{documents}\"\n",
    "\n",
    "Дополнительный блок ссылок:\n",
    "https://developers.sber.ru/docs/ru/gigachat/api/overview - документация по API\n",
    "https://github.com/ai-forever/gigachain - репозиторий GigaChain на GitHub с исходными кодами SDK и примерами\n",
    "https://developers.sber.ru/docs/ru/gigachain/overview - документация по GigaChain\n",
    "https://developers.sber.ru/docs/ru/gigachain/gigagraph/overview - документация по GigaGraph\n",
    "https://www.youtube.com/watch?v=HAg-GFKl1rc&ab_channel=SaluteTech - видео \"быстрый старт по работе с GigaChat API за 1 минуту\"\n",
    "https://developers.sber.ru/help/gigachat-api - база знаний по gigachat api\n",
    "https://courses.sberuniversity.ru/llm-gigachat/ - курс по LLM GigaChat\n",
    "\n",
    "Ты должен всегда отвечать в таком JSON формате:\n",
    "{{\n",
    "\"thought\": \"твои рассуждения по поводу оценки Опиши пошагово своё рассуждение, чтобы убедиться, что твой вывод правильный\",\n",
    "\"answered_on_documents\": 0 или 1, где 0 — ответ не основан на документах; 1 — ответ основан на документах,\n",
    "\"answer_has_links\": 0 или 1, где 0 - ответ не содержит релативные ссылки; 1 — ответ содержит релативные ссылки,\n",
    "}}\n",
    "\n",
    "Начнём!\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from langchain.evaluation import CotQAEvalChain\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Используйте мощную модель для лучшего сравнения ответов\n",
    "eval_llm = ChatOpenAI(temperature=0, model=\"gpt-4o-2024-08-06\")\n",
    "\n",
    "answered_on_documents_chain = ANSWERED_ON_DOCUMENTS_PROMPT | eval_llm | JsonOutputParser()\n",
    "cot_chain = CotQAEvalChain.from_llm(llm=eval_llm, prompt=COT_PROMPT)\n",
    "\n",
    "async def evaluation(query, output, expected_output, documents):\n",
    "    resp1 = cot_chain._prepare_output(await cot_chain.ainvoke({\n",
    "        \"query\": query, \"context\": expected_output, \"result\": output\n",
    "    }))\n",
    "    thought = f\"{resp1['reasoning']}\"\n",
    "    score = resp1['score']\n",
    "    avg_score = score\n",
    "    has_links = 0\n",
    "    on_documents = 0\n",
    "    # Добавляем оценку наличия ссылок и соответствия информации из документов, только при наличии документов\n",
    "    # Если документов нет, то мы оцениваем скорее всего small-talk ответы\n",
    "    # или цепочку без RAG\n",
    "    if documents:\n",
    "        resp2 = await answered_on_documents_chain.with_retry().ainvoke({\n",
    "            \"answer\": output, \"documents\": documents\n",
    "        })\n",
    "        has_links = resp2['answer_has_links']\n",
    "        on_documents = resp2['answered_on_documents']\n",
    "        avg_score += has_links + on_documents\n",
    "        avg_score /= 3\n",
    "        thought += f\"\\n-----\\n{resp2['thought']}\"\n",
    "    return {\n",
    "        'reasoning': thought,\n",
    "        'avg_score': avg_score,\n",
    "        'cot_llm': score,\n",
    "        'has_links': has_links,\n",
    "        'on_documents': on_documents\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Проверим работу цепочки оценки ответов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": "{'reasoning': 'EXPLANATION: Чтобы оценить ответ студента, сначала нужно определить, что требуется в вопросе. Вопрос спрашивает о главном герое книги. Правильный ответ на этот вопрос — \"Собака\". Теперь сравним это с ответом студента, который утверждает, что главный герой — \"Кот\". Поскольку ответ студента не совпадает с правильным ответом и указывает на другого персонажа, это делает его ответ неверным. В данном случае, ответ студента не соответствует фактической информации, представленной в правильном ответе.\\n\\nGRADE: INCORRECT',\n 'avg_score': 0,\n 'cot_llm': 0,\n 'has_links': 0,\n 'on_documents': 0}"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Тут оценка неправильного ответа от LLM\n",
    "await evaluation(\"Кто главный герой книги\", \"Кот\", \"Собака\", [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": "{'reasoning': '1. Вопрос спрашивает о главном герое книги.\\n2. Корректный ответ на вопрос — \"Котик\".\\n3. Ответ студента — \"Кот\".\\n4. Сравнивая оба ответа, можно заметить, что \"Кот\" и \"Котик\" очень близки по значению. \"Котик\" может быть уменьшительно-ласкательной формой слова \"Кот\".\\n5. В данном контексте, оба слова указывают на одно и то же животное, и нет противоречий между ответом студента и правильным ответом.\\n6. Таким образом, ответ студента можно считать правильным, так как он передает ту же самую информацию, что и правильный ответ.\\n\\nGRADE: CORRECT',\n 'avg_score': 1,\n 'cot_llm': 1,\n 'has_links': 0,\n 'on_documents': 0}"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Тут оценка правильного ответа от LLM\n",
    "await evaluation(\"Кто главный герой книги\", \"Кот\", \"Котик\", [])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Оценка\n",
    "### Оценка ответов с обычным GigaChat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from langchain_community.chat_models import GigaChat\n",
    "llm = GigaChat(model=\"GigaChat-Pro\", temperature=0.01, profanity_check=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Мы оцениваем 3 ответа на один вопрос датасета, для получения средней оценки.\n",
    "Из-за того, что цепочки могут иметь внутри себя компоненты с запросами к LLM\n",
    "где температура не равна 0, нам нужно получить несколько раз ответы, чтобы получить среднюю оценку."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [06:29<00:00,  5.56s/it]\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "from tqdm.asyncio import tqdm\n",
    "\n",
    "dataset = langfuse.get_dataset(\"rag_dataset\")\n",
    "\n",
    "async def without_rag(item, run_name, semaphore, retries=3):\n",
    "    async with semaphore:\n",
    "        for _ in range(retries):\n",
    "            handler = item.get_langchain_handler(run_name=run_name)\n",
    "            try:\n",
    "                generation = (await llm.ainvoke(input=item.input, config={\"callbacks\": [handler]})).content\n",
    "                resp = await evaluation(item.input, generation, item.expected_output, [])\n",
    "                handler.trace.score(\n",
    "                    name=\"avg_score\",\n",
    "                    value=resp['avg_score'],\n",
    "                    comment=resp['reasoning']\n",
    "                )\n",
    "                for score_name in ['cot_llm', 'has_links', 'on_documents']:\n",
    "                    handler.trace.score(\n",
    "                        name=score_name,\n",
    "                        value=resp[score_name]\n",
    "                    )\n",
    "            except Exception as e:\n",
    "                handler.trace.score(\n",
    "                    name=\"avg_score\",\n",
    "                    value=0,\n",
    "                    comment=str(e)\n",
    "                )\n",
    "                for score_name in ['cot_llm', 'has_links', 'on_documents']:\n",
    "                    handler.trace.score(\n",
    "                        name=score_name,\n",
    "                        value=0\n",
    "                    )\n",
    "\n",
    "tasks = []\n",
    "sem = asyncio.Semaphore(10)\n",
    "name = f\"llm_without_rag\"\n",
    "\n",
    "for item in dataset.items:\n",
    "    tasks.append(without_rag(item, name, sem))\n",
    "\n",
    "r = await tqdm.gather(*tasks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Первый прогон сделан. Смотрим результат...\n",
    "![скриншот прогона](media/llm_without_rag.png)\n",
    "Результат вышел `0.15`.\n",
    "Судя по всему GigaChat хорошо справляется с вопросами сам о себе, но про GigaChain отвечает слабо.\n",
    "Теперь попробуем прогнать датасет с простым RAG\n",
    "### Оценка ответов GigaChat + RAG(стандартный)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "from graph import vector_store"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableParallel\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.chains.question_answering.stuff_prompt import CHAT_PROMPT\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "rag_chain_from_docs = (\n",
    "    RunnablePassthrough.assign(context=(lambda x: format_docs(x[\"context\"])))\n",
    "    | CHAT_PROMPT\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "rag_chain_with_source = RunnableParallel(\n",
    "    {\"context\": vector_store.as_retriever(), \"question\": RunnablePassthrough()}\n",
    ").assign(answer=rag_chain_from_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'Выполните команду bash pip install -U gigachain_community'"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain_with_source.invoke(\"Как обновить GigaChain?\")['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:00<00:00, 409485.75it/s]\n",
      "100%|██████████| 70/70 [11:47<00:00, 10.11s/it]\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "from tqdm.asyncio import tqdm\n",
    "dataset = langfuse.get_dataset(\"rag_dataset\")\n",
    "\n",
    "async def with_rag(item, run_name, semaphore, retries=3):\n",
    "    async with semaphore:\n",
    "        for _ in range(retries):\n",
    "            handler = item.get_langchain_handler(run_name=run_name)\n",
    "            try:\n",
    "                generation = await rag_chain_with_source.ainvoke(input=item.input, config={\"callbacks\": [handler]})\n",
    "                resp = await evaluation(item.input, generation['answer'], item.expected_output, generation['context'])\n",
    "                handler.trace.score(\n",
    "                    name=\"avg_score\",\n",
    "                    value=resp['avg_score'],\n",
    "                    comment=resp['reasoning']\n",
    "                )\n",
    "                for score_name in ['cot_llm', 'has_links', 'on_documents']:\n",
    "                    handler.trace.score(\n",
    "                        name=score_name,\n",
    "                        value=resp[score_name]\n",
    "                    )\n",
    "            except Exception as e:\n",
    "                handler.trace.score(\n",
    "                    name=\"avg_score\",\n",
    "                    value=0,\n",
    "                    comment=str(e)\n",
    "                )\n",
    "                for score_name in ['cot_llm', 'has_links', 'on_documents']:\n",
    "                    handler.trace.score(\n",
    "                        name=score_name,\n",
    "                        value=0\n",
    "                    )\n",
    "\n",
    "tasks = []\n",
    "sem = asyncio.Semaphore(5)\n",
    "name = f\"llm_with_rag\"\n",
    "\n",
    "for item in tqdm(dataset.items):\n",
    "    tasks.append(with_rag(item, name, sem))\n",
    "\n",
    "r = await tqdm.gather(*tasks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Смотрим результат...\n",
    "![скриншот прогона](media/llm_with_rag.png)\n",
    "Результат вышел `0.46`.\n",
    "### Оценка ответов GigaChat + Adaptive RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "from graph import graph, GraphState"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'Для обновления GigaChain до последней версии можно использовать команду `bash pip install -U gigachain_community` в терминале. Это обеспечит установку последних исправлений и улучшений в библиотеке.'"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(await graph.ainvoke(input=GraphState(question=\"Как обновить GigaChain?\")))['generation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:00<00:00, 165037.26it/s]\n",
      " 64%|██████▍   | 45/70 [09:17<03:28,  8.35s/it] Giga generation stopped with reason: blacklist\n",
      "100%|██████████| 70/70 [14:21<00:00, 12.30s/it]\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "from tqdm.asyncio import tqdm\n",
    "dataset = langfuse.get_dataset(\"rag_dataset\")\n",
    "\n",
    "async def with_arag(item, run_name, semaphore, retries=3):\n",
    "    async with semaphore:\n",
    "        for _ in range(retries):\n",
    "            handler = item.get_langchain_handler(run_name=run_name)\n",
    "            try:\n",
    "                s = GraphState(question=item.input)\n",
    "                generation = await graph.ainvoke(input=s, config={\"callbacks\": [handler]})\n",
    "                resp = await evaluation(item.input, generation['generation'], item.expected_output, generation.get(\"documents\", []))\n",
    "                handler.trace.score(\n",
    "                    name=\"avg_score\",\n",
    "                    value=resp['avg_score'],\n",
    "                    comment=resp['reasoning']\n",
    "                )\n",
    "                for score_name in ['cot_llm', 'has_links', 'on_documents']:\n",
    "                    handler.trace.score(\n",
    "                        name=score_name,\n",
    "                        value=resp[score_name]\n",
    "                    )\n",
    "            except Exception as e:\n",
    "                handler.trace.score(\n",
    "                    name=\"avg_score\",\n",
    "                    value=0,\n",
    "                    comment=str(e)\n",
    "                )\n",
    "                for score_name in ['cot_llm', 'has_links', 'on_documents']:\n",
    "                    handler.trace.score(\n",
    "                        name=score_name,\n",
    "                        value=0\n",
    "                    )\n",
    "\n",
    "tasks = []\n",
    "sem = asyncio.Semaphore(5)\n",
    "name = f\"llm_with_arag\"\n",
    "\n",
    "for item in tqdm(dataset.items):\n",
    "    tasks.append(with_arag(item, name, sem))\n",
    "\n",
    "r = await tqdm.gather(*tasks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Смотрим результат...\n",
    "![скриншот прогона](media/llm_with_arag.png)\n",
    "Результат вышел `0.38`.\n",
    "Почему?\n",
    "Дело в том, что ARAG сам выбирает относиться ли вопрос к нашей векторной базе данных,\n",
    "и может отказаться от ответа, не обращаясь к ней. Здесь качество зависит от качества промпта\n",
    "который направляет запрос в графе.\n",
    "### Оценка ответов Support Bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "from graph_2 import graph as graph_2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:00<00:00, 243047.42it/s]\n",
      "100%|██████████| 70/70 [14:57<00:00, 12.82s/it]\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "from tqdm.asyncio import tqdm\n",
    "dataset = langfuse.get_dataset(\"rag_dataset\")\n",
    "\n",
    "async def with_arag(item, run_name, semaphore, retries=3):\n",
    "    async with semaphore:\n",
    "        for _ in range(retries):\n",
    "            handler = item.get_langchain_handler(run_name=run_name)\n",
    "            try:\n",
    "                s = GraphState(question=item.input)\n",
    "                generation = await graph_2.ainvoke(input=s, config={\"callbacks\": [handler]})\n",
    "                resp = await evaluation(item.input, generation['generation'], item.expected_output, generation.get(\"documents\", []))\n",
    "                handler.trace.score(\n",
    "                    name=\"avg_score\",\n",
    "                    value=resp['avg_score'],\n",
    "                    comment=resp['reasoning']\n",
    "                )\n",
    "                for score_name in ['cot_llm', 'has_links', 'on_documents']:\n",
    "                    handler.trace.score(\n",
    "                        name=score_name,\n",
    "                        value=resp[score_name]\n",
    "                    )\n",
    "            except Exception as e:\n",
    "                handler.trace.score(\n",
    "                    name=\"avg_score\",\n",
    "                    value=0,\n",
    "                    comment=str(e)\n",
    "                )\n",
    "                for score_name in ['cot_llm', 'has_links', 'on_documents']:\n",
    "                    handler.trace.score(\n",
    "                        name=score_name,\n",
    "                        value=0\n",
    "                    )\n",
    "\n",
    "tasks = []\n",
    "sem = asyncio.Semaphore(5)\n",
    "name = f\"llm_with_support_bot\"\n",
    "\n",
    "for item in tqdm(dataset.items):\n",
    "    tasks.append(with_arag(item, name, sem))\n",
    "\n",
    "r = await tqdm.gather(*tasks)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Смотрим результат...\n",
    "![скриншот прогона](media/llm_with_support_bot.png)\n",
    "Результат вышел `0.79`."
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
