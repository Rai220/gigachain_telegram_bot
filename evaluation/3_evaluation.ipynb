{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Evaluation в GigaLogger\n",
    "В этом ноутбуке мы произведем оценку нашего RAG'а с помощью датасета и мощной LLM (gpt-4o)\n",
    "И не только! Мы также замерим качество ответов на обычном GigaChat (без RAG), с обычным RAG, Adaptive RAG и нашу версию RAG.\n",
    "У нас в боте используется измененный Adaptive RAG.\n",
    "Предыдущие шаги:\n",
    "1. [Генерация синтетического датасета](1_generate_dataset.ipynb)\n",
    "2. [Загрузка датасета в GigaLogger](2_gigalogger_create_dataset.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import getpass\n",
    "\n",
    "def get_env_var(var_name):\n",
    "    if var_name in os.environ:\n",
    "        return os.environ[var_name]\n",
    "    else:\n",
    "        return getpass.getpass(f\"Enter {var_name}: \")\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")  # Add the parent folder to the sys.path\n",
    "\n",
    "load_dotenv(find_dotenv())\n",
    "os.environ[\"LANGFUSE_HOST\"] = \"https://gigalogger.demo.sberdevices.ru\"\n",
    "os.environ[\"LANGFUSE_PUBLIC_KEY\"] = get_env_var(\"LANGFUSE_PUBLIC_KEY\")\n",
    "os.environ[\"LANGFUSE_SECRET_KEY\"] = get_env_var(\"LANGFUSE_SECRET_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from langfuse import Langfuse\n",
    "langfuse = Langfuse()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Цепочка для оценки ответов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Определим промпты для оценки ответов\n",
    "Мы будем оценивать по следующим критериям:\n",
    "- Похожи ли ответ нашей цепочки и корректный ответ (из датасета)\n",
    "- Содержит ли ответ информацию из документов, которые мы нашли с помощью RAG\n",
    "- Есть ли в ответе ссылки из документов (или из стандартного раздела ссылок)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "COT_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"query\", \"context\", \"result\"], template=\"\"\"Ты учитель, оценивающий тест.\n",
    "\n",
    "Тебе дан вопрос, корректный ответ и ответ студента. Тебе нужно оценить ответ студента как ПРАВИЛЬНЫЙ или НЕПРАВИЛЬНЫЙ, основываясь на корректном ответе.\n",
    "Опиши пошагово своё рассуждение, чтобы убедиться, что твой вывод правильный. Избегай просто указывать правильный ответ с самого начала.\n",
    "\n",
    "Вот базовая информация из конкретной области этого теста:\n",
    "GigaChat - это большая языковая модель (LLM) от Сбера.\n",
    "GigaChat API (апи) - это API для взаимодействия с GigaChat по HTTP с помощью REST запросов.\n",
    "GigaChain - это SDK на Python для работы с GigaChat API. Русскоязычный форк библиотеки LangChain.\n",
    "GigaGraph - это дополнение для GigaChain, который позволяет создавать мультиагентные системы, описывая их в виде графов.\n",
    "Обучение GigaChat выполняется командой разработчиков. Дообучение и файнтюнинг для конечных пользователей на данный момент не доступно.\n",
    "Для получения доступа к API нужно зарегистрироваться на developers.sber.ru и получить авторизационные данные.\n",
    "\n",
    "Опирайся на эту базовую информацию, если тебе не хватает информации для проверки теста.\n",
    "\n",
    "Пример формата:\n",
    "QUESTION: здесь вопрос\n",
    "TRUE ANSWER: здесь корректный ответ\n",
    "STUDENT ANSWER: здесь ответ студента\n",
    "EXPLANATION: пошаговое рассуждение здесь\n",
    "GRADE: CORRECT или INCORRECT здесь\n",
    "\n",
    "Тебе будем дан только один ответ студента, не несколько.\n",
    "Оценивай ответ студента ТОЛЬКО на основе их фактической точности. \n",
    "Игнорируй различия в пунктуации и формулировках между ответом студента и правильным ответом.\n",
    "Ответ студента может содержать больше информации, чем правильный ответ, если в нём нет противоречивых утверждений, то он корректен. Начнём!\n",
    "\n",
    "QUESTION: \"{query}\"\n",
    "TRUE ANSWER: \"{context}\"\n",
    "STUDENT ANSWER: \"{result}\"\n",
    "EXPLANATION:\"\"\"\n",
    ")\n",
    "\n",
    "ANSWERED_ON_DOCUMENTS_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"answer\", \"documents\"], template=\"\"\"Ты учитель, оценивающий тест.\n",
    "Тебе будет дан ответ студента и документы, которые были даны студенту.\n",
    "Избегай просто указывать правильный ответ с самого начала.\n",
    "Ты должен оценить ответ студента исходя из следующих критериев:\n",
    "* Ответ студента основан на документах, которые были даны студенту\n",
    "* Ответ студента содержит ссылки из документов, относящихся к вопросу или ссылки из дополнительного блока ссылок\n",
    "\n",
    "Ответ студента: \"{answer}\"\n",
    "Документы: \"{documents}\"\n",
    "\n",
    "Ты должен всегда отвечать в таком JSON формате:\n",
    "{{\n",
    "\"thought\": \"Твои рассуждения по поводу оценки. Опиши пошагово своё рассуждение, чтобы убедиться, что твой вывод правильный\",\n",
    "\"answered_on_documents\": 0 или 1, где 0 — ответ не основан на документах; 1 — ответ основан на документах,\n",
    "\"answer_has_links\": 0 или 1, где 0 - ответ не содержит релативные ссылки; 1 — ответ содержит релативные ссылки,\n",
    "}}\n",
    "\n",
    "Начнём!\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from langchain.evaluation import CotQAEvalChain\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Используйте мощную модель для лучшего сравнения ответов\n",
    "eval_llm = ChatOpenAI(temperature=0, model=\"gpt-4o-2024-08-06\")\n",
    "\n",
    "answered_on_documents_chain = ANSWERED_ON_DOCUMENTS_PROMPT | eval_llm | JsonOutputParser()\n",
    "cot_chain = CotQAEvalChain.from_llm(llm=eval_llm, prompt=COT_PROMPT)\n",
    "\n",
    "async def evaluation(query, output, expected_output, documents):\n",
    "    resp1 = cot_chain._prepare_output(await cot_chain.ainvoke({\n",
    "        \"query\": query, \"context\": expected_output, \"result\": output\n",
    "    }))\n",
    "    thought = f\"{resp1['reasoning']}\"\n",
    "    score = resp1['score']\n",
    "    avg_score = score\n",
    "    has_links = 0\n",
    "    on_documents = 0\n",
    "    # Добавляем оценку наличия ссылок и соответствия информации из документов, только при наличии документов\n",
    "    # Если документов нет, то мы оцениваем скорее всего small-talk ответы\n",
    "    # или цепочку без RAG\n",
    "    if documents:\n",
    "        resp2 = await answered_on_documents_chain.with_retry().ainvoke({\n",
    "            \"answer\": output, \"documents\": documents\n",
    "        })\n",
    "        # Вес оценки со ссылками - 0.1\n",
    "        has_links = resp2['answer_has_links'] / 10\n",
    "        on_documents = resp2['answered_on_documents']\n",
    "        avg_score += has_links + on_documents\n",
    "        avg_score /= 2.1\n",
    "        thought += f\"\\n-----\\n{resp2['thought']}\"\n",
    "    return {\n",
    "        'reasoning': thought,\n",
    "        'avg_score': avg_score,\n",
    "        'cot_llm': score,\n",
    "        'has_links': has_links,\n",
    "        'on_documents': on_documents\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Проверим работу цепочки оценки ответов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'reasoning': 'EXPLANATION: Чтобы оценить ответ студента, сначала нужно определить, что требуется в вопросе. Вопрос спрашивает о главном герое книги. Правильный ответ на этот вопрос — \"Собака\". Теперь сравним это с ответом студента, который утверждает, что главный герой — \"Кот\". Поскольку ответ студента не совпадает с правильным ответом и указывает на другого персонажа, это делает его ответ неверным. В данном случае, ответ студента не соответствует фактической информации, представленной в правильном ответе.\\n\\nGRADE: INCORRECT',\n",
       " 'avg_score': 0,\n",
       " 'cot_llm': 0,\n",
       " 'has_links': 0,\n",
       " 'on_documents': 0}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Тут оценка неправильного ответа от LLM\n",
    "await evaluation(query=\"Кто главный герой книги\", output=\"Кот\", expected_output=\"Собака\", documents=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'reasoning': 'EXPLANATION: Чтобы оценить ответ студента, нужно сравнить его с корректным ответом. В данном случае, корректный ответ - \"Котик\". Ответ студента - \"Кот\". Оба ответа указывают на одно и то же существо, однако в ответе студента используется более обобщённое название. Важно определить, является ли это различие критичным для понимания и точности ответа. В данном контексте, \"Кот\" и \"Котик\" могут быть восприняты как одно и то же, если в книге не делается акцент на уменьшительно-ласкательной форме имени. Поскольку в вопросе не указано, что форма имени имеет значение, можно считать, что ответ студента передаёт ту же информацию, что и правильный ответ.\\n\\nGRADE: CORRECT',\n",
       " 'avg_score': 1,\n",
       " 'cot_llm': 1,\n",
       " 'has_links': 0,\n",
       " 'on_documents': 0}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Тут оценка правильного ответа от LLM\n",
    "await evaluation(\"Кто главный герой книги\", \"Кот\", \"Котик\", [])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Внимание!\n",
    "Возможно скоро COT Chain цепочка устареет и станет deprecated, поэтому здесь [another_cot_chain.ipynb](another_cot_chain.ipynb)\n",
    "вы можете найти более свежий пример данной цепочки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Оценка\n",
    "### Функция оценки\n",
    "\n",
    "Мы будем оценивать качество ответов по следующим параметрам:\n",
    "Корректность ответа (0-1 балл)\n",
    "Основан ли ответ на документах? (0-1 балл)\n",
    "Содержит ли ответ ссылки на документы? (0-0.1 балл)\n",
    "\n",
    "Далее оценка суммируется и нормируется таким образом, чтобы суммарная оценка была от 0 до 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/19563044/Documents/giga/gigachain_telegram_bot/venv/lib/python3.12/site-packages/pinecone/data/index.py:1: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "from tqdm.asyncio import tqdm\n",
    "from typing import Any, Coroutine\n",
    "from langchain.schema import BaseMessage\n",
    "from langchain.schema import AIMessage\n",
    "from graph import graph, GraphState\n",
    "\n",
    "\n",
    "async def evaluate(run_name: str, generator: Coroutine[Any, Any, BaseMessage], graph=False):\n",
    "    dataset = langfuse.get_dataset(\"rag_dataset\")\n",
    "\n",
    "    async def without_rag(item, run_name, semaphore, retries=1):\n",
    "        async with semaphore:\n",
    "            for _ in range(retries):\n",
    "                handler = item.get_langchain_handler(run_name=run_name)\n",
    "                try:\n",
    "                    inp = item\n",
    "                    if graph:\n",
    "                        inp = GraphState(question=item.input)\n",
    "                    else:\n",
    "                        inp = item.input\n",
    "                        \n",
    "                    generation = (await generator.ainvoke(input=inp, config={\"callbacks\": [handler]}))\n",
    "                    answer = \"\"\n",
    "                    context = []\n",
    "                    if isinstance(generation, str):\n",
    "                        answer = generation\n",
    "                    elif isinstance(generation, AIMessage):\n",
    "                        answer = generation.content\n",
    "                    else:\n",
    "                        answer = generation.get('answer', generation.get('generation', \"\"))\n",
    "                        context = generation.get('context', generation.get('documents', []))\n",
    "                    resp = await evaluation(input, answer, item.expected_output, context)\n",
    "                    \n",
    "                    handler.trace.score(\n",
    "                        name=\"avg_score\",\n",
    "                        value=resp['avg_score'],\n",
    "                        comment=resp['reasoning']\n",
    "                    )\n",
    "                    for score_name in ['cot_llm', 'has_links', 'on_documents']:\n",
    "                        handler.trace.score(\n",
    "                            name=score_name,\n",
    "                            value=resp[score_name]\n",
    "                        )\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "                    handler.trace.score(\n",
    "                        name=\"avg_score\",\n",
    "                        value=0,\n",
    "                        comment=str(e)\n",
    "                    )\n",
    "                    for score_name in ['cot_llm', 'has_links', 'on_documents']:\n",
    "                        handler.trace.score(\n",
    "                            name=score_name,\n",
    "                            value=0\n",
    "                        )\n",
    "\n",
    "    tasks = []\n",
    "    sem = asyncio.Semaphore(5)\n",
    "\n",
    "    for item in dataset.items:\n",
    "        tasks.append(without_rag(item, run_name, sem))\n",
    "\n",
    "    r = await tqdm.gather(*tasks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Оценка GigaChat lite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Обновление GigaChain может включать несколько шагов в зависимости от версии и типа обновления. Вот общий процесс для обновления GigaChain:\\n\\n1. **Проверка текущей версии**: Убедитесь, что у вас установлена правильная версия GigaChain. Это можно сделать с помощью команды `gchain version`.\\n\\n2. **Подготовка к обновлению**:\\n   - Создайте резервную копию базы данных и всех конфигурационных файлов.\\n   - Остановите все сервисы GigaChain (например, `gserver` и `gwallet`).\\n\\n3. **Скачивание новой версии**: Перейдите на официальный сайт GigaChain и скачайте последнюю версию.\\n\\n4. **Обновление кода**: Распакуйте скачанный архив и замените существующие файлы и директории новыми.\\n\\n5. **Конфигурация**: Если в новой версии изменилась структура конфигурации, настройте её соответствующим образом.\\n\\n6. **Запуск сервисов**: Запустите сервер и кошелек снова (`gserver`, `gwallet`), чтобы убедиться, что всё работает корректно.\\n\\n7. **Тестирование**: Проведите тестирование обновленной системы, чтобы убедиться в её работоспособности.\\n\\nЕсли у вас возникнут какие-либо проблемы или вопросы по ходу процесса, рекомендуется обратиться к документации GigaChain или связаться с поддержкой проекта.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.chat_models import GigaChat\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "giga = GigaChat(model=\"GigaChat\", temperature=1e-15, profanity_check=False, max_tokens=8000)\n",
    "chain_giga_lite = giga | StrOutputParser()\n",
    "\n",
    "chain_giga_lite.invoke(\"Как обновить GigaChain?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████▉  | 59/74 [03:12<00:44,  2.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 66/74 [03:39<00:23,  2.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 74/74 [04:10<00:00,  3.38s/it]\n"
     ]
    }
   ],
   "source": [
    "await evaluate(\"gigachat_lite\", chain_giga_lite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "giga_pro = GigaChat(model=\"GigaChat-Pro\", temperature=0.00001, profanity_check=False, max_tokens=8000)\n",
    "await evaluate(\"gigachat_pro\", giga_pro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0, model=\"gpt-4o-2024-08-06\")\n",
    "await evaluate(\"gpt-4o\", llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь мы можем зайти в интерфейс GigaLogger и увидеть получившиеся оценки - 0.11 для gigachat lite и 0.15 для gigachat-pro."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](media/llm_without_rag.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Судя по всему GigaChat хорошо справляется с вопросами сам о себе, но про GigaChain отвечает слабо.\n",
    "Теперь попробуем прогнать датасет с простым RAG\n",
    "\n",
    "### Оценка ответов GigaChat + RAG(стандартный)\n",
    "Для начала инициализируем векторную базу данных. В этом примере используется внешняя БД Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain_community.embeddings.gigachat import GigaChatEmbeddings\n",
    "from pinecone import Pinecone\n",
    "\n",
    "pinecone_api_key = os.environ.get(\"PINECONE_API_KEY\")\n",
    "\n",
    "pc = Pinecone(api_key=pinecone_api_key)\n",
    "index = pc.Index(\"gigachain-test-index-gigar\")\n",
    "\n",
    "embeddings = GigaChatEmbeddings(model=\"EmbeddingsGigaR\")\n",
    "vector_store = PineconeVectorStore(index=index, embedding=embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Собираем цепочку с RAG\n",
    "### Классический RAG без документов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.chains.question_answering.stuff_prompt import CHAT_PROMPT\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "vector_store.as_retriever().invoke(\"Как обновить GigaChain?\")\n",
    "\n",
    "rag_chain = (\n",
    "    {\n",
    "        \"context\": vector_store.as_retriever() | format_docs,\n",
    "        \"question\": RunnablePassthrough(),\n",
    "    }\n",
    "    | CHAT_PROMPT\n",
    "    | giga_pro\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Выполните команду bash pip install -U gigachain_community'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\"Как обновить GigaChain?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'context': [Document(metadata={'source': 'https://giga.chat/help/articles/faq'}, page_content='Как обновить GigaChain?\\n\\nВыполните команду bash pip install -U gigachain_community\\n\\n## Как установить gigachain\\n\\nВыполните команду bash pip install gigachain_community\\n\\n## Как обновиться с langchain на gigachain Нужно создать чистое виртуальное окружение python и активировать его. Для Mac/Linux, например так: bash python -m venv venv source venv/bin/activate\\n\\nДалее можно установить gigachain: bash pip install gigachain_community\\n\\nРучное удаление langchain не рекомендуется.\\n\\n## Как установить LangGraph bash pip install langgraph'),\n",
       "  Document(metadata={'source': 'https://courses.sberuniversity.ru/llm-gigachat/'}, page_content='4.1 Что такое GigaChain и как его установить\\n\\nВведение\\n\\nGigaChain SDK — это библиотека инструментов для упрощения и автоматизации работы c GigaChat. Информация из этого урока и далее будет полезна, прежде всего, разработчикам, которые занимаются интеграцией GigaChat с продуктами для бизнеса.\\n\\nGigaChain — это версия на базе библиотеки LangChain для работы с русским языком, что позволяет использовать её при работе и с другими языковыми моделями.\\n\\nБиблиотека стандартизирует типовые кейсы использования языковых моделей (работа с цепочками, базами знаний и документами) и содержит набор готовых промптов для решения бизнес-задач.\\n\\nДо этого момента мы показывали взаимодействие с GigaChat API на тестовой платформе Postman. Вообще же системный промптинг — это написание команд с помощью языка программирования. В курсе мы приводим разбор инструментов и задач с иллюстрациями, но, чтобы самостоятельно протестировать методы из урока, вам потребуется войти в среду разработки, например, PyCharm.\\n\\nВ этом уроке вы узнаете, как установить библиотеку GigaChain, работать с промптами и пользоваться хабом готовых промптов.\\n\\nКак установить и пользоваться GigaChain\\n\\nИтак, GigaСhain – это ответвление (fork) открытой библиотеки LangСhain на Python. В библиотеке много различных утилит и компонентов для работы с промптами. Базовый объект GigaChain — цепочки, последовательности вызовов к модели и другим инструментам.\\n\\nВ GigaChat SDK вы найдёте:\\n\\nБиблиотеку, которая содержит интерфейсы и интеграции для разных компонентов, базовую среду выполнения для объединения этих компонентов в цепочки и агенты, готовые реализации цепочек и агентов.\\n\\nКаталог (хаб) промптов. Набор типовых отлаженных промптов для решения различных задач.\\n\\nGigaChain Templates. Это коллекция легко развёртываемых шаблонных решений для широкого спектра задач.\\n\\nGigaServe. Библиотека, позволяющая публиковать цепочки GigaChain в форме REST API.'),\n",
       "  Document(metadata={'source': 'https://github.com/ai-forever/gigachain'}, page_content=\"Подробнее о том, как внести свой вклад.\\n\\n📖 Дополнительная документация\\n\\n[!NOTE] Полная документация GigaChain находится в процессе перевода. Вы можете также пользоваться документацией LangChain, поскольку GigaChain совместим с LangChain:\\n\\nIntroduction: Overview of the framework and the structure of the docs.\\n\\nTutorials: If you're looking to build something specific or are more of a hands-on learner, check out our tutorials. This is the best place to get started.\\n\\nHow-to guides: Answers to “How do I….?” type questions. These guides are goal-oriented and concrete; they're meant to help you complete a specific task.\\n\\nConceptual guide: Conceptual explanations of the key parts of the framework.\\n\\nAPI Reference: Thorough documentation of every class and method.\\n\\nЛицензия\\n\\nПроект распространяется по лицензии MIT, доступной в файле LICENSE.\\n\\n[^1]: В настоящий момент эта функциональность доступна в бета-режиме.\"),\n",
       "  Document(metadata={'source': 'https://github.com/ai-forever/gigachain'}, page_content='🦜️🔗 GigaChain (GigaChat + LangChain)\\n\\nБиблиотека для разработки LangChain-style приложений на русском языке с поддержкой GigaChat Создать issue · Документация GigaChain\\n\\n🤔 Что такое GigaChain?\\n\\nGigaChain это фреймворк для разработки приложений с использованием больших языковых моделей (LLM), таких, как GigaChat или YandexGPT. Он позволяет создавать приложения, которые:\\n\\nУчитывают контекст — подключите свою модель к источникам данных.\\n\\nМогут рассуждать — положитесь на модель в построении рассуждениях (о том, как ответить, опираясь на контекст, какие действия предпринять и т.д.).\\n\\n[!WARNING] Версия библиотеки LangChain адаптированная для русского языка с поддержкой нейросетевой модели GigaChat. Библиотека GigaChain обратно совместима с LangChain, что позволяет использовать ее не только для работы с GigaChat, но и при работе с другими LLM в различных комбинациях.\\n\\nФреймворк включает:\\n\\nБиблиотеку GigaChain. Библиотека на Python содержит интерфейсы и интеграции для множества компонентов, базовую среду выполнения для объединения этих компонентов в цепочки и агенты, а также готовые реализации цепочек и агентов.\\n\\nХаб промптов. Набор типовых отлаженных промптов для решения различных задач.\\n\\nGigaChain Templates. Коллекция легко развертываемых шаблонных решений для широкого спектра задач.\\n\\nGigaServe. Библиотека, позволяющая публиковать цепочки GigaChain в форме REST API.\\n\\nGigaGraph. Библиотека, дающая возможность работать с LLM (большими языковыми моделями), для создания приложений, которые используют множество взаимодействующих цепочек (акторов) и сохраняют данные о состоянии. Так как в основе GigaGraph лежит GigaChain, предполагается совместное использование обеих библиотек.\\n\\nКроме этого, фреймворк совместим со сторонним сервисом LangSmith — платформой для разработчиков, которая позволяет отлаживать, тестировать, оценивать и отслеживать цепочки, построенные на любой платформе LLM, и легко интегрируется с LangChain и GigaChain.\\n\\nРепозиторий содержит следующие компоненты:')],\n",
       " 'question': 'Как обновить GigaChain?',\n",
       " 'answer': 'Выполните команду bash pip install -U gigachain_community'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableParallel\n",
    "\n",
    "rag_chain = (\n",
    "    RunnablePassthrough.assign(context=(lambda x: format_docs(x[\"context\"])))\n",
    "    | CHAT_PROMPT\n",
    "    | giga_pro\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "rag_chain_with_source = RunnableParallel(\n",
    "    {\"context\": vector_store.as_retriever(), \"question\": RunnablePassthrough()}\n",
    ").assign(answer=rag_chain)\n",
    "\n",
    "rag_chain_with_source.invoke(\"Как обновить GigaChain?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 74/74 [06:55<00:00,  5.62s/it]\n"
     ]
    }
   ],
   "source": [
    "await evaluate(\"rag_gigar\", rag_chain_with_source)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "![image-2.png](media/llm_with_rag.jpeg)\n",
    "\n",
    "Оценка - 0.71\n",
    "\n",
    "### Оценка ответов GigaChat + Adaptive RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/19563044/Documents/giga/gigachain_telegram_bot/venv/lib/python3.12/site-packages/pinecone/data/index.py:1: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Конечно, вот улучшенный ответ:\n",
      "\n",
      "Для обновления GigaChain выполните следующую команду в терминале:\n",
      "```bash\n",
      "pip install -U gigachain_community\n",
      "```\n",
      "Это обновит вашу установку до последней версии. Если у вас есть другие вопросы или нужна помощь, пожалуйста, дайте знать!\n"
     ]
    }
   ],
   "source": [
    "from graph import graph, GraphState\n",
    "\n",
    "generation = await graph.ainvoke(input=GraphState(question=\"Как обновить GigaChain?\"))\n",
    "print(generation['generation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 40/74 [02:26<01:32,  2.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(URL('https://wmapi-ift.saluteai-pd.sberdevices.ru/v1/chat/completions'), 500, b'{\"status\":500,\"message\":\"Internal Server Error\"}\\n', Headers({'server': 'nginx', 'date': 'Tue, 24 Sep 2024 08:00:32 GMT', 'content-type': 'application/json; charset=utf-8', 'content-length': '49', 'connection': 'keep-alive', 'keep-alive': 'timeout=15', 'access-control-allow-credentials': 'true', 'access-control-allow-headers': 'Origin, X-Requested-With, Content-Type, Accept, Authorization', 'access-control-allow-methods': 'GET, POST, DELETE, OPTIONS', 'access-control-allow-origin': 'https://beta.saluteai.sberdevices.ru', 'x-request-id': '7a9f80a6-c5d8-4d89-b43d-56e891cef00a', 'x-session-id': '7b38898a-c810-4e73-92cc-8f8a6f627bc5', 'x-sp-crid': '1851564027:2'}))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 74/74 [04:09<00:00,  3.37s/it]\n"
     ]
    }
   ],
   "source": [
    "await evaluate(\"arag_gigar\", graph, graph=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Смотрим результат...\n",
    "![image-2.png](media/llm_with_arag.jpeg)\n",
    "Результат вышел `0.59`, меньше чем у просто RAG.\n",
    "Почему?\n",
    "Дело в том, что ARAG сам выбирает относиться ли вопрос к нашей векторной базе данных,\n",
    "и может отказаться от ответа, не обращаясь к ней. Здесь качество зависит от качества промпта\n",
    "который направляет запрос в графе.\n",
    "### Оценка ответов Support Bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Для обновления GigaChain выполните команду `bash pip install -U gigachain_community`. Дополнительную информацию и примеры использования GigaChain можно найти в следующих ресурсах:\\n\\n- Документация по API: [https://developers.sber.ru/docs/ru/gigachat/api/overview](https://developers.sber.ru/docs/ru/gigachat/api/overview)\\n- Репозиторий GigaChain на GitHub: [https://github.com/ai-forever/gigachain](https://github.com/ai-forever/gigachain)\\n- Курс по LLM GigaChat: [https://courses.sberuniversity.ru/llm-gigachat/](https://courses.sberuniversity.ru/llm-gigachat/)\\n\\nЭти ресурсы предоставят подробные руководства и примеры кода для работы с GigaChain.'"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from graph_2 import graph as graph_2\n",
    "(await graph_2.ainvoke(input=GraphState(question=\"Как обновить GigaChain?\")))['generation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 74/74 [16:07<00:00, 13.07s/it]\n"
     ]
    }
   ],
   "source": [
    "await evaluate(\"support_bot_v2_gigar_2\", graph_2, graph=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Смотрим результат...\n",
    "![image.png](media/llm_with_support_bot.jpeg)\n",
    "Результат вышел `0.75`.\n",
    "На данный момент эта версия дает наилучшее качество"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Версия Support RAG v3 - доработанная версия для telegram-бота"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Для обновления GigaChain выполните команду `pip install -U gigachain_community` в вашем терминале или командной строке. [Дополнительная информация об установке и обновлении GigaChain](https://github.com/ai-forever/gigachain).'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from graph_3 import graph as graph_3\n",
    "(await graph_3.ainvoke(input=GraphState(question=\"Как обновить GigaChain?\")))['generation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await evaluate(\"support_bot_v3_gigachat_max\", graph_3, graph=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
