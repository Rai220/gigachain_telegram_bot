{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Evaluation в GigaLogger\n",
    "В этом ноутбуке мы произведем оценку нашего RAG'а с помощью датасета и мощной LLM (gpt-4o)\n",
    "И не только! Мы также замерим качество ответов на обычном GigaChat (без RAG), с обычным RAG, Adaptive RAG и нашу версию RAG.\n",
    "У нас в боте используется измененный Adaptive RAG.\n",
    "Предыдущие шаги:\n",
    "1. [Генерация синтетического датасета](1_generate_dataset.ipynb)\n",
    "2. [Загрузка датасета в GigaLogger](2_gigalogger_create_dataset.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import getpass\n",
    "\n",
    "def get_env_var(var_name):\n",
    "    if var_name in os.environ:\n",
    "        return os.environ[var_name]\n",
    "    else:\n",
    "        return getpass.getpass(f\"Enter {var_name}: \")\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")  # Add the parent folder to the sys.path\n",
    "\n",
    "load_dotenv(find_dotenv())\n",
    "os.environ[\"LANGFUSE_HOST\"] = \"https://gigalogger.demo.sberdevices.ru\"\n",
    "os.environ[\"LANGFUSE_PUBLIC_KEY\"] = get_env_var(\"LANGFUSE_PUBLIC_KEY\")\n",
    "os.environ[\"LANGFUSE_SECRET_KEY\"] = get_env_var(\"LANGFUSE_SECRET_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# !pip install langfuse\n",
    "from langfuse import Langfuse\n",
    "langfuse = Langfuse()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Цепочка для оценки ответов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Определим промпты для оценки ответов\n",
    "Мы будем оценивать по следующим критериям:\n",
    "- Похожи ли ответ нашей цепочки и корректный ответ (из датасета)\n",
    "- Содержит ли ответ информацию из документов, которые мы нашли с помощью RAG\n",
    "- Есть ли в ответе ссылки из документов (или из стандартного раздела ссылок)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "COT_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"query\", \"context\", \"result\"], template=\"\"\"Ты учитель, оценивающий тест.\n",
    "\n",
    "Тебе дан вопрос, корректный ответ и ответ студента. Тебе нужно оценить ответ студента как ПРАВИЛЬНЫЙ или НЕПРАВИЛЬНЫЙ, основываясь на корректном ответе.\n",
    "Опиши пошагово своё рассуждение, чтобы убедиться, что твой вывод правильный. Избегай просто указывать правильный ответ с самого начала.\n",
    "\n",
    "Вот базовая информация из конкретной области этого теста:\n",
    "GigaChat - это большая языковая модель (LLM) от Сбера.\n",
    "GigaChat API (апи) - это API для взаимодействия с GigaChat по HTTP с помощью REST запросов.\n",
    "GigaChain - это SDK на Python для работы с GigaChat API. Русскоязычный форк библиотеки LangChain.\n",
    "GigaGraph - это дополнение для GigaChain, который позволяет создавать мультиагентные системы, описывая их в виде графов.\n",
    "Обучение GigaChat выполняется командой разработчиков. Дообучение и файнтюнинг для конечных пользователей на данный момент не доступно.\n",
    "Для получения доступа к API нужно зарегистрироваться на developers.sber.ru и получить авторизационные данные.\n",
    "\n",
    "Опирайся на эту базовую информацию, если тебе не хватает информации для проверки теста.\n",
    "\n",
    "Пример формата:\n",
    "QUESTION: здесь вопрос\n",
    "TRUE ANSWER: здесь корректный ответ\n",
    "STUDENT ANSWER: здесь ответ студента\n",
    "EXPLANATION: пошаговое рассуждение здесь\n",
    "GRADE: CORRECT или INCORRECT здесь\n",
    "\n",
    "Тебе будем дан только один ответ студента, не несколько.\n",
    "Оценивай ответ студента ТОЛЬКО на основе их фактической точности. \n",
    "Игнорируй различия в пунктуации и формулировках между ответом студента и правильным ответом.\n",
    "Ответ студента может содержать больше информации, чем правильный ответ, если в нём нет противоречивых утверждений, то он корректен. Начнём!\n",
    "\n",
    "QUESTION: \"{query}\"\n",
    "TRUE ANSWER: \"{context}\"\n",
    "STUDENT ANSWER: \"{result}\"\n",
    "EXPLANATION:\"\"\"\n",
    ")\n",
    "\n",
    "ANSWERED_ON_DOCUMENTS_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"answer\", \"documents\"], template=\"\"\"Ты учитель, оценивающий тест.\n",
    "Тебе будет дан ответ студента и документы, которые были даны студенту.\n",
    "Избегай просто указывать правильный ответ с самого начала.\n",
    "Ты должен оценить ответ студента исходя из следующих критериев:\n",
    "* Ответ студента основан на документах, которые были даны студенту\n",
    "* Ответ студента содержит ссылки из документов, относящихся к вопросу или ссылки из дополнительного блока ссылок\n",
    "\n",
    "Ответ студента: \"{answer}\"\n",
    "Документы: \"{documents}\"\n",
    "\n",
    "Ты должен всегда отвечать в таком JSON формате:\n",
    "{{\n",
    "\"thought\": \"Твои рассуждения по поводу оценки. Опиши пошагово своё рассуждение, чтобы убедиться, что твой вывод правильный\",\n",
    "\"answered_on_documents\": 0 или 1, где 0 — ответ не основан на документах; 1 — ответ основан на документах,\n",
    "\"answer_has_links\": 0 или 1, где 0 - ответ не содержит релативные ссылки; 1 — ответ содержит релативные ссылки,\n",
    "}}\n",
    "\n",
    "Начнём!\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from langchain.evaluation import CotQAEvalChain\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Используйте мощную модель для лучшего сравнения ответов\n",
    "eval_llm = ChatOpenAI(temperature=0, model=\"gpt-4o-2024-08-06\")\n",
    "\n",
    "answered_on_documents_chain = ANSWERED_ON_DOCUMENTS_PROMPT | eval_llm | JsonOutputParser()\n",
    "cot_chain = CotQAEvalChain.from_llm(llm=eval_llm, prompt=COT_PROMPT)\n",
    "\n",
    "async def evaluation(query, output, expected_output, documents):\n",
    "    resp1 = cot_chain._prepare_output(await cot_chain.ainvoke({\n",
    "        \"query\": query, \"context\": expected_output, \"result\": output\n",
    "    }))\n",
    "    thought = f\"{resp1['reasoning']}\"\n",
    "    score = resp1['score']\n",
    "    avg_score = score\n",
    "    has_links = 0\n",
    "    on_documents = 0\n",
    "    # Добавляем оценку наличия ссылок и соответствия информации из документов, только при наличии документов\n",
    "    # Если документов нет, то мы оцениваем скорее всего small-talk ответы\n",
    "    # или цепочку без RAG\n",
    "    if documents:\n",
    "        resp2 = await answered_on_documents_chain.with_retry().ainvoke({\n",
    "            \"answer\": output, \"documents\": documents\n",
    "        })\n",
    "        # Вес оценки со ссылками - 0.1\n",
    "        has_links = resp2['answer_has_links'] / 10\n",
    "        on_documents = resp2['answered_on_documents']\n",
    "        avg_score += has_links + on_documents\n",
    "        avg_score /= 2.1\n",
    "        thought += f\"\\n-----\\n{resp2['thought']}\"\n",
    "    return {\n",
    "        'reasoning': thought,\n",
    "        'avg_score': avg_score,\n",
    "        'cot_llm': score,\n",
    "        'has_links': has_links,\n",
    "        'on_documents': on_documents\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Проверим работу цепочки оценки ответов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'reasoning': 'EXPLANATION: Чтобы оценить ответ студента, сначала нужно определить, что требуется в вопросе. Вопрос спрашивает о главном герое книги \"Тестовая книга\". Согласно корректному ответу, главным героем является \"Собака\". Теперь сравним это с ответом студента, который утверждает, что главным героем является \"Кот\". Поскольку ответ студента не совпадает с корректным ответом и указывает на другого персонажа, он является неправильным. В данном случае, ответ студента не соответствует фактической информации, предоставленной в корректном ответе.\\n\\nGRADE: INCORRECT',\n",
       " 'avg_score': 0,\n",
       " 'cot_llm': 0,\n",
       " 'has_links': 0,\n",
       " 'on_documents': 0}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Тут оценка неправильного ответа от LLM\n",
    "await evaluation(query=\"Кто главный герой книги `Тестовая книга`?\", output=\"Кот\", expected_output=\"Собака\", documents=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'reasoning': '1. Прежде всего, я сравниваю ответ студента с корректным ответом. В данном случае, корректный ответ - \"Котик\", а ответ студента - \"Кот\".\\n\\n2. Оба ответа указывают на одно и то же животное, но используют разные формы слова. \"Котик\" - это уменьшительно-ласкательная форма слова \"Кот\". \\n\\n3. В контексте вопроса о главном герое книги, использование уменьшительно-ласкательной формы не меняет сути ответа. Оба ответа указывают на одно и то же существо.\\n\\n4. Важно, что ответ студента не содержит противоречивой информации и соответствует сути правильного ответа.\\n\\n5. Таким образом, ответ студента можно считать правильным, так как он указывает на того же персонажа, что и корректный ответ.\\n\\nGRADE: CORRECT',\n",
       " 'avg_score': 1,\n",
       " 'cot_llm': 1,\n",
       " 'has_links': 0,\n",
       " 'on_documents': 0}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Тут оценка правильного ответа от LLM\n",
    "await evaluation(\"Кто главный герой книги `Тестовая книга`?\", \"Кот\", \"Котик\", [])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Внимание!\n",
    "Возможно скоро COT Chain цепочка устареет и станет deprecated, поэтому здесь [another_cot_chain.ipynb](another_cot_chain.ipynb)\n",
    "вы можете найти более свежий пример данной цепочки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Оценка\n",
    "### Функция оценки\n",
    "\n",
    "Мы будем оценивать качество ответов по следующим параметрам:\n",
    "Корректность ответа (0-1 балл)\n",
    "Основан ли ответ на документах? (0-1 балл)\n",
    "Содержит ли ответ ссылки на документы? (0-0.1 балл)\n",
    "\n",
    "Далее оценка суммируется и нормируется таким образом, чтобы суммарная оценка была от 0 до 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from tqdm.asyncio import tqdm\n",
    "from typing import Any, Coroutine\n",
    "from langchain.schema import BaseMessage\n",
    "from langchain.schema import AIMessage\n",
    "from graph import graph, GraphState\n",
    "\n",
    "\n",
    "async def evaluate(run_name: str, generator: Coroutine[Any, Any, BaseMessage], graph=False):\n",
    "    dataset = langfuse.get_dataset(\"rag_dataset\")\n",
    "\n",
    "    async def without_rag(item, run_name, semaphore, retries=1):\n",
    "        async with semaphore:\n",
    "            for _ in range(retries):\n",
    "                handler = item.get_langchain_handler(run_name=run_name)\n",
    "                try:\n",
    "                    inp = item\n",
    "                    if graph:\n",
    "                        inp = GraphState(question=item.input)\n",
    "                    else:\n",
    "                        inp = item.input\n",
    "                        \n",
    "                    generation = (await generator.ainvoke(input=inp, config={\"callbacks\": [handler]}))\n",
    "                    answer = \"\"\n",
    "                    context = []\n",
    "                    if isinstance(generation, str):\n",
    "                        answer = generation\n",
    "                    elif isinstance(generation, AIMessage):\n",
    "                        answer = generation.content\n",
    "                    else:\n",
    "                        answer = generation.get('answer', generation.get('generation', \"\"))\n",
    "                        context = generation.get('context', generation.get('documents', []))\n",
    "                    resp = await evaluation(input, answer, item.expected_output, context)\n",
    "                    \n",
    "                    handler.trace.score(\n",
    "                        name=\"avg_score\",\n",
    "                        value=resp['avg_score'],\n",
    "                        comment=resp['reasoning']\n",
    "                    )\n",
    "                    for score_name in ['cot_llm', 'has_links', 'on_documents']:\n",
    "                        handler.trace.score(\n",
    "                            name=score_name,\n",
    "                            value=resp[score_name]\n",
    "                        )\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "                    handler.trace.score(\n",
    "                        name=\"avg_score\",\n",
    "                        value=0,\n",
    "                        comment=str(e)\n",
    "                    )\n",
    "                    for score_name in ['cot_llm', 'has_links', 'on_documents']:\n",
    "                        handler.trace.score(\n",
    "                            name=score_name,\n",
    "                            value=0\n",
    "                        )\n",
    "\n",
    "    tasks = []\n",
    "    sem = asyncio.Semaphore(5)\n",
    "\n",
    "    for item in dataset.items:\n",
    "        tasks.append(without_rag(item, run_name, sem))\n",
    "\n",
    "    r = await tqdm.gather(*tasks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Оценка GigaChat lite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Обновление блокчейн-сети GigaChain может включать несколько шагов и зависит от конкретной версии сети и используемого программного обеспечения. Вот общий алгоритм действий для обновления GigaChain:\\n\\n### 1. **Проверка текущей версии**\\n   Убедитесь, что у вас установлена последняя версия GigaChain. Для этого можно использовать команду `gchain version` в консоли. Если версия устарела, переходите к следующему шагу.\\n\\n### 2. **Скачивание последней версии**\\n   Перейдите на официальный сайт GigaChain или воспользуйтесь репозиторием GitHub, чтобы скачать последнюю версию программы.\\n\\n### 3. **Остановка текущего узла**\\n   Остановите работу вашего текущего узла с помощью команды `gnode stop`. Это важно, чтобы избежать конфликтов при замене файлов.\\n\\n### 4. **Удаление старой версии**\\n   Если вы работаете с локальной копией, удалите папку с предыдущей версией GigaChain. Если используется Docker или другой контейнерный подход, уберите все связанные образы и контейнеры.\\n\\n### 5. **Распаковка новой версии**\\n   Распакуйте архив с новой версией в ту же директорию, где была старая версия.\\n\\n### 6. **Запуск нового узла (если необходимо)**\\n   Запустите новый узел с помощью `gnode start`, если это требуется.\\n\\nЕсли вы используете Docker или другие инструменты контейнеризации, процесс будет немного отличаться. В этом случае вам нужно обновить образ Docker или изменить конфигурацию контейнера.\\n\\nДля более точной информации о процессе обновления конкретной реализации GigaChain лучше обратиться к документации проекта или разработчикам.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.chat_models import GigaChat\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "giga = GigaChat(model=\"GigaChat\", top_p=0, profanity_check=False, max_tokens=8000)\n",
    "chain_giga_lite = giga | StrOutputParser()\n",
    "\n",
    "chain_giga_lite.invoke(\"Как обновить GigaChain?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 74/74 [03:30<00:00,  2.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "await evaluate(\"gigachat_lite_2024_06_11(2)\", chain_giga_lite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 9/74 [00:31<01:49,  1.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 74/74 [03:49<00:00,  3.11s/it]\n"
     ]
    }
   ],
   "source": [
    "giga_pro = GigaChat(model=\"GigaChat-Pro\", top_p=0, profanity_check=False, max_tokens=8000)\n",
    "await evaluate(\"gigachat_pro_2024_06_11\", giga_pro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|██▉       | 22/74 [01:07<02:54,  3.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 28/74 [01:27<01:59,  2.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 58/74 [03:00<00:56,  3.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 74/74 [03:47<00:00,  3.07s/it]\n"
     ]
    }
   ],
   "source": [
    "giga_pro = GigaChat(model=\"GigaChat-Pro\", top_p=0, profanity_check=False, max_tokens=8000)\n",
    "await evaluate(\"gigachat_max_2024_06_11\", giga_pro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 74/74 [03:26<00:00,  2.78s/it]\n"
     ]
    }
   ],
   "source": [
    "llm = ChatOpenAI(temperature=0, model=\"gpt-4o\")\n",
    "await evaluate(\"gpt-4o\", llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь мы можем зайти в интерфейс GigaLogger и увидеть получившиеся оценки - 0.11 для gigachat lite и 0.15 для gigachat-pro."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](media/llm_without_rag.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Судя по всему GigaChat хорошо справляется с вопросами сам о себе, но про GigaChain отвечает слабо.\n",
    "Теперь попробуем прогнать датасет с простым RAG\n",
    "\n",
    "### Оценка ответов GigaChat + RAG(стандартный)\n",
    "Для начала инициализируем векторную базу данных. В этом примере используется внешняя БД Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain_community.embeddings.gigachat import GigaChatEmbeddings\n",
    "from pinecone import Pinecone\n",
    "\n",
    "pinecone_api_key = os.environ.get(\"PINECONE_API_KEY\")\n",
    "\n",
    "pc = Pinecone(api_key=pinecone_api_key)\n",
    "index = pc.Index(\"gigachain-test-index-gigar\")\n",
    "\n",
    "embeddings = GigaChatEmbeddings(model=\"EmbeddingsGigaR\")\n",
    "vector_store = PineconeVectorStore(index=index, embedding=embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Собираем цепочку с RAG\n",
    "### Классический RAG без документов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.chains.question_answering.stuff_prompt import CHAT_PROMPT\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "vector_store.as_retriever().invoke(\"Как обновить GigaChain?\")\n",
    "\n",
    "rag_chain = (\n",
    "    {\n",
    "        \"context\": vector_store.as_retriever() | format_docs,\n",
    "        \"question\": RunnablePassthrough(),\n",
    "    }\n",
    "    | CHAT_PROMPT\n",
    "    | giga_pro\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Выполните команду bash pip install -U gigachain_community'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\"Как обновить GigaChain?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'context': [Document(metadata={'source': 'https://giga.chat/help/articles/faq'}, page_content='Как обновить GigaChain?\\n\\nВыполните команду bash pip install -U gigachain_community\\n\\n## Как установить gigachain\\n\\nВыполните команду bash pip install gigachain_community\\n\\n## Как обновиться с langchain на gigachain Нужно создать чистое виртуальное окружение python и активировать его. Для Mac/Linux, например так: bash python -m venv venv source venv/bin/activate\\n\\nДалее можно установить gigachain: bash pip install gigachain_community\\n\\nРучное удаление langchain не рекомендуется.\\n\\n## Как установить LangGraph bash pip install langgraph'),\n",
       "  Document(metadata={'source': 'https://courses.sberuniversity.ru/llm-gigachat/'}, page_content='4.1 Что такое GigaChain и как его установить\\n\\nВведение\\n\\nGigaChain SDK — это библиотека инструментов для упрощения и автоматизации работы c GigaChat. Информация из этого урока и далее будет полезна, прежде всего, разработчикам, которые занимаются интеграцией GigaChat с продуктами для бизнеса.\\n\\nGigaChain — это версия на базе библиотеки LangChain для работы с русским языком, что позволяет использовать её при работе и с другими языковыми моделями.\\n\\nБиблиотека стандартизирует типовые кейсы использования языковых моделей (работа с цепочками, базами знаний и документами) и содержит набор готовых промптов для решения бизнес-задач.\\n\\nДо этого момента мы показывали взаимодействие с GigaChat API на тестовой платформе Postman. Вообще же системный промптинг — это написание команд с помощью языка программирования. В курсе мы приводим разбор инструментов и задач с иллюстрациями, но, чтобы самостоятельно протестировать методы из урока, вам потребуется войти в среду разработки, например, PyCharm.\\n\\nВ этом уроке вы узнаете, как установить библиотеку GigaChain, работать с промптами и пользоваться хабом готовых промптов.\\n\\nКак установить и пользоваться GigaChain\\n\\nИтак, GigaСhain – это ответвление (fork) открытой библиотеки LangСhain на Python. В библиотеке много различных утилит и компонентов для работы с промптами. Базовый объект GigaChain — цепочки, последовательности вызовов к модели и другим инструментам.\\n\\nВ GigaChat SDK вы найдёте:\\n\\nБиблиотеку, которая содержит интерфейсы и интеграции для разных компонентов, базовую среду выполнения для объединения этих компонентов в цепочки и агенты, готовые реализации цепочек и агентов.\\n\\nКаталог (хаб) промптов. Набор типовых отлаженных промптов для решения различных задач.\\n\\nGigaChain Templates. Это коллекция легко развёртываемых шаблонных решений для широкого спектра задач.\\n\\nGigaServe. Библиотека, позволяющая публиковать цепочки GigaChain в форме REST API.'),\n",
       "  Document(metadata={'source': 'https://github.com/ai-forever/gigachain'}, page_content=\"Подробнее о том, как внести свой вклад.\\n\\n📖 Дополнительная документация\\n\\n[!NOTE] Полная документация GigaChain находится в процессе перевода. Вы можете также пользоваться документацией LangChain, поскольку GigaChain совместим с LangChain:\\n\\nIntroduction: Overview of the framework and the structure of the docs.\\n\\nTutorials: If you're looking to build something specific or are more of a hands-on learner, check out our tutorials. This is the best place to get started.\\n\\nHow-to guides: Answers to “How do I….?” type questions. These guides are goal-oriented and concrete; they're meant to help you complete a specific task.\\n\\nConceptual guide: Conceptual explanations of the key parts of the framework.\\n\\nAPI Reference: Thorough documentation of every class and method.\\n\\nЛицензия\\n\\nПроект распространяется по лицензии MIT, доступной в файле LICENSE.\\n\\n[^1]: В настоящий момент эта функциональность доступна в бета-режиме.\"),\n",
       "  Document(metadata={'source': 'https://github.com/ai-forever/gigachain'}, page_content='🦜️🔗 GigaChain (GigaChat + LangChain)\\n\\nБиблиотека для разработки LangChain-style приложений на русском языке с поддержкой GigaChat Создать issue · Документация GigaChain\\n\\n🤔 Что такое GigaChain?\\n\\nGigaChain это фреймворк для разработки приложений с использованием больших языковых моделей (LLM), таких, как GigaChat или YandexGPT. Он позволяет создавать приложения, которые:\\n\\nУчитывают контекст — подключите свою модель к источникам данных.\\n\\nМогут рассуждать — положитесь на модель в построении рассуждениях (о том, как ответить, опираясь на контекст, какие действия предпринять и т.д.).\\n\\n[!WARNING] Версия библиотеки LangChain адаптированная для русского языка с поддержкой нейросетевой модели GigaChat. Библиотека GigaChain обратно совместима с LangChain, что позволяет использовать ее не только для работы с GigaChat, но и при работе с другими LLM в различных комбинациях.\\n\\nФреймворк включает:\\n\\nБиблиотеку GigaChain. Библиотека на Python содержит интерфейсы и интеграции для множества компонентов, базовую среду выполнения для объединения этих компонентов в цепочки и агенты, а также готовые реализации цепочек и агентов.\\n\\nХаб промптов. Набор типовых отлаженных промптов для решения различных задач.\\n\\nGigaChain Templates. Коллекция легко развертываемых шаблонных решений для широкого спектра задач.\\n\\nGigaServe. Библиотека, позволяющая публиковать цепочки GigaChain в форме REST API.\\n\\nGigaGraph. Библиотека, дающая возможность работать с LLM (большими языковыми моделями), для создания приложений, которые используют множество взаимодействующих цепочек (акторов) и сохраняют данные о состоянии. Так как в основе GigaGraph лежит GigaChain, предполагается совместное использование обеих библиотек.\\n\\nКроме этого, фреймворк совместим со сторонним сервисом LangSmith — платформой для разработчиков, которая позволяет отлаживать, тестировать, оценивать и отслеживать цепочки, построенные на любой платформе LLM, и легко интегрируется с LangChain и GigaChain.\\n\\nРепозиторий содержит следующие компоненты:')],\n",
       " 'question': 'Как обновить GigaChain?',\n",
       " 'answer': 'Выполните команду bash pip install -U gigachain_community'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableParallel\n",
    "\n",
    "rag_chain = (\n",
    "    RunnablePassthrough.assign(context=(lambda x: format_docs(x[\"context\"])))\n",
    "    | CHAT_PROMPT\n",
    "    | giga_pro\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "rag_chain_with_source = RunnableParallel(\n",
    "    {\"context\": vector_store.as_retriever(), \"question\": RunnablePassthrough()}\n",
    ").assign(answer=rag_chain)\n",
    "\n",
    "rag_chain_with_source.invoke(\"Как обновить GigaChain?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 74/74 [06:55<00:00,  5.62s/it]\n"
     ]
    }
   ],
   "source": [
    "await evaluate(\"rag_gigar\", rag_chain_with_source)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "![image-2.png](media/llm_with_rag.jpeg)\n",
    "\n",
    "Оценка - 0.71\n",
    "\n",
    "### Оценка ответов GigaChat + Adaptive RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Для обновления GigaChain до последней версии необходимо выполнить команду `pip install -U gigachain_community` в вашем терминале. Это обновление установит последнюю версию SDK, предоставляя вам доступ к новым функциям и улучшениям. Если у вас возникнут дополнительные вопросы или проблемы при обновлении, пожалуйста, обратитесь к документации по GigaChain на сайте developers.sber.ru или воспользуйтесь разделом помощи на том же ресурсе.\n"
     ]
    }
   ],
   "source": [
    "from graph import graph, GraphState\n",
    "\n",
    "generation = await graph.ainvoke(input=GraphState(question=\"Как обновить GigaChain?\"))\n",
    "print(generation['generation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 40/74 [02:26<01:32,  2.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(URL('https://wmapi-ift.saluteai-pd.sberdevices.ru/v1/chat/completions'), 500, b'{\"status\":500,\"message\":\"Internal Server Error\"}\\n', Headers({'server': 'nginx', 'date': 'Tue, 24 Sep 2024 08:00:32 GMT', 'content-type': 'application/json; charset=utf-8', 'content-length': '49', 'connection': 'keep-alive', 'keep-alive': 'timeout=15', 'access-control-allow-credentials': 'true', 'access-control-allow-headers': 'Origin, X-Requested-With, Content-Type, Accept, Authorization', 'access-control-allow-methods': 'GET, POST, DELETE, OPTIONS', 'access-control-allow-origin': 'https://beta.saluteai.sberdevices.ru', 'x-request-id': '7a9f80a6-c5d8-4d89-b43d-56e891cef00a', 'x-session-id': '7b38898a-c810-4e73-92cc-8f8a6f627bc5', 'x-sp-crid': '1851564027:2'}))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 74/74 [04:09<00:00,  3.37s/it]\n"
     ]
    }
   ],
   "source": [
    "await evaluate(\"arag_gigar\", graph, graph=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Смотрим результат...\n",
    "![image-2.png](media/llm_with_arag.jpeg)\n",
    "Результат вышел `0.59`, меньше чем у просто RAG.\n",
    "Почему?\n",
    "Дело в том, что ARAG сам выбирает относиться ли вопрос к нашей векторной базе данных,\n",
    "и может отказаться от ответа, не обращаясь к ней. Здесь качество зависит от качества промпта\n",
    "который направляет запрос в графе.\n",
    "### Оценка ответов Support Bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Для обновления GigaChain выполните команду `bash pip install -U gigachain_community`. Дополнительную информацию и примеры использования GigaChain можно найти в следующих ресурсах:\\n\\n- Документация по API: [https://developers.sber.ru/docs/ru/gigachat/api/overview](https://developers.sber.ru/docs/ru/gigachat/api/overview)\\n- Репозиторий GigaChain на GitHub: [https://github.com/ai-forever/gigachain](https://github.com/ai-forever/gigachain)\\n- Курс по LLM GigaChat: [https://courses.sberuniversity.ru/llm-gigachat/](https://courses.sberuniversity.ru/llm-gigachat/)\\n\\nЭти ресурсы предоставят подробные руководства и примеры кода для работы с GigaChain.'"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from graph_2 import graph as graph_2\n",
    "(await graph_2.ainvoke(input=GraphState(question=\"Как обновить GigaChain?\")))['generation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 74/74 [16:07<00:00, 13.07s/it]\n"
     ]
    }
   ],
   "source": [
    "await evaluate(\"support_bot_v2_gigar_2\", graph_2, graph=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Смотрим результат...\n",
    "![image.png](media/llm_with_support_bot.jpeg)\n",
    "Результат вышел `0.75`.\n",
    "На данный момент эта версия дает наилучшее качество"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Версия Support RAG v3 - доработанная версия для telegram-бота"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Для обновления GigaChain выполните команду `bash pip install -U gigachain_community`. \\n\\nДополнительную информацию вы можете найти здесь:\\n- [FAQ по GigaChat](https://giga.chat/help/articles/faq)\\n- [Документация по GigaChain](https://github.com/ai-forever/gigachain)\\n\\nОбратите внимание, что GigaChain находится в стадии альфа-версии, поэтому будьте осторожны при использовании его в ваших проектах.'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from graph import graph, GraphState\n",
    "\n",
    "from graph_3 import graph as graph_3\n",
    "(await graph_3.ainvoke(input=GraphState(question=\"Как обновить GigaChain?\")))['generation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 1/74 [00:04<05:37,  4.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'original_question'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 20/74 [01:22<02:53,  3.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'original_question'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 28/74 [01:42<01:48,  2.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'original_question'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 45/74 [02:34<00:50,  1.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'documents'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 46/74 [02:35<00:40,  1.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'documents'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 56/74 [03:11<00:46,  2.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'original_question'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 62/74 [03:31<00:32,  2.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'original_question'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 74/74 [04:15<00:00,  3.45s/it]\n"
     ]
    }
   ],
   "source": [
    "await evaluate(\"support_bot_v3_gpt4_openai_embeddings(2)\", graph_3, graph=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top! - Ø 0.8713"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
