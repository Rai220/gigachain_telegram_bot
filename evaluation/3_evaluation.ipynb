{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Evaluation в GigaLogger\n",
    "В этом ноутбуке мы произведем оценку нашего RAG'а с помощью датасета и мощной LLM (gpt-4o)\n",
    "И не только! Мы также замерим качество ответов на обычном GigaChat (без RAG), с обычным RAG, Adaptive RAG и нашу версию RAG.\n",
    "У нас в боте используется измененный Adaptive RAG.\n",
    "Предыдущие шаги:\n",
    "1. [Генерация синтетического датасета](1_generate_dataset.ipynb)\n",
    "2. [Загрузка датасета в GigaLogger](2_gigalogger_create_dataset.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import getpass\n",
    "\n",
    "def get_env_var(var_name):\n",
    "    if var_name in os.environ:\n",
    "        return os.environ[var_name]\n",
    "    else:\n",
    "        return getpass.getpass(f\"Enter {var_name}: \")\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")  # Add the parent folder to the sys.path\n",
    "\n",
    "load_dotenv(find_dotenv())\n",
    "os.environ[\"LANGFUSE_HOST\"] = \"https://gigalogger.demo.sberdevices.ru\"\n",
    "os.environ[\"LANGFUSE_PUBLIC_KEY\"] = get_env_var(\"LANGFUSE_PUBLIC_KEY\")\n",
    "os.environ[\"LANGFUSE_SECRET_KEY\"] = get_env_var(\"LANGFUSE_SECRET_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from langfuse import Langfuse\n",
    "langfuse = Langfuse()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Цепочка для оценки ответов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Определим промпты для оценки ответов\n",
    "Мы будем оценивать по следующим критериям:\n",
    "- Похожи ли ответ нашей цепочки и корректный ответ (из датасета)\n",
    "- Содержит ли ответ информацию из документов, которые мы нашли с помощью RAG\n",
    "- Есть ли в ответе ссылки из документов (или из стандартного раздела ссылок)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "COT_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"query\", \"context\", \"result\"], template=\"\"\"Ты учитель, оценивающий тест.\n",
    "\n",
    "Тебе дан вопрос, корректный ответ и ответ студента. Тебе нужно оценить ответ студента как ПРАВИЛЬНЫЙ или НЕПРАВИЛЬНЫЙ, основываясь на корректном ответе.\n",
    "Опиши пошагово своё рассуждение, чтобы убедиться, что твой вывод правильный. Избегай просто указывать правильный ответ с самого начала.\n",
    "\n",
    "Вот базовая информация из конкретной области этого теста:\n",
    "GigaChat - это большая языковая модель (LLM) от Сбера.\n",
    "GigaChat API (апи) - это API для взаимодействия с GigaChat по HTTP с помощью REST запросов.\n",
    "GigaChain - это SDK на Python для работы с GigaChat API. Русскоязычный форк библиотеки LangChain.\n",
    "GigaGraph - это дополнение для GigaChain, который позволяет создавать мультиагентные системы, описывая их в виде графов.\n",
    "Обучение GigaChat выполняется командой разработчиков. Дообучение и файнтюнинг для конечных пользователей на данный момент не доступно.\n",
    "Для получения доступа к API нужно зарегистрироваться на developers.sber.ru и получить авторизационные данные.\n",
    "\n",
    "Опирайся на эту базовую информацию, если тебе не хватает информации для проверки теста.\n",
    "\n",
    "Пример формата:\n",
    "QUESTION: здесь вопрос\n",
    "TRUE ANSWER: здесь корректный ответ\n",
    "STUDENT ANSWER: здесь ответ студента\n",
    "EXPLANATION: пошаговое рассуждение здесь\n",
    "GRADE: CORRECT или INCORRECT здесь\n",
    "\n",
    "Тебе будем дан только один ответ студента, не несколько.\n",
    "Оценивай ответ студента ТОЛЬКО на основе их фактической точности. \n",
    "Игнорируй различия в пунктуации и формулировках между ответом студента и правильным ответом.\n",
    "Ответ студента может содержать больше информации, чем правильный ответ, если в нём нет противоречивых утверждений, то он корректен. Начнём!\n",
    "\n",
    "QUESTION: \"{query}\"\n",
    "TRUE ANSWER: \"{context}\"\n",
    "STUDENT ANSWER: \"{result}\"\n",
    "EXPLANATION:\"\"\"\n",
    ")\n",
    "\n",
    "ANSWERED_ON_DOCUMENTS_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"answer\", \"documents\"], template=\"\"\"Ты учитель, оценивающий тест.\n",
    "Тебе будет дан ответ студента и документы, которые были даны студенту.\n",
    "Избегай просто указывать правильный ответ с самого начала.\n",
    "Ты должен оценить ответ студента исходя из следующих критериев:\n",
    "* Ответ студента основан на документах, которые были даны студенту\n",
    "* Ответ студента содержит ссылки из документов, относящихся к вопросу или ссылки из дополнительного блока ссылок\n",
    "\n",
    "Ответ студента: \"{answer}\"\n",
    "Документы: \"{documents}\"\n",
    "\n",
    "Ты должен всегда отвечать в таком JSON формате:\n",
    "{{\n",
    "\"thought\": \"Твои рассуждения по поводу оценки. Опиши пошагово своё рассуждение, чтобы убедиться, что твой вывод правильный\",\n",
    "\"answered_on_documents\": 0 или 1, где 0 — ответ не основан на документах; 1 — ответ основан на документах,\n",
    "\"answer_has_links\": 0 или 1, где 0 - ответ не содержит релативные ссылки; 1 — ответ содержит релативные ссылки,\n",
    "}}\n",
    "\n",
    "Начнём!\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from langchain.evaluation import CotQAEvalChain\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Используйте мощную модель для лучшего сравнения ответов\n",
    "eval_llm = ChatOpenAI(temperature=0, model=\"gpt-4o-2024-08-06\")\n",
    "\n",
    "answered_on_documents_chain = ANSWERED_ON_DOCUMENTS_PROMPT | eval_llm | JsonOutputParser()\n",
    "cot_chain = CotQAEvalChain.from_llm(llm=eval_llm, prompt=COT_PROMPT)\n",
    "\n",
    "async def evaluation(query, output, expected_output, documents):\n",
    "    resp1 = cot_chain._prepare_output(await cot_chain.ainvoke({\n",
    "        \"query\": query, \"context\": expected_output, \"result\": output\n",
    "    }))\n",
    "    thought = f\"{resp1['reasoning']}\"\n",
    "    score = resp1['score']\n",
    "    avg_score = score\n",
    "    has_links = 0\n",
    "    on_documents = 0\n",
    "    # Добавляем оценку наличия ссылок и соответствия информации из документов, только при наличии документов\n",
    "    # Если документов нет, то мы оцениваем скорее всего small-talk ответы\n",
    "    # или цепочку без RAG\n",
    "    if documents:\n",
    "        resp2 = await answered_on_documents_chain.with_retry().ainvoke({\n",
    "            \"answer\": output, \"documents\": documents\n",
    "        })\n",
    "        # Вес оценки со ссылками - 0.1\n",
    "        has_links = resp2['answer_has_links'] / 10\n",
    "        on_documents = resp2['answered_on_documents']\n",
    "        avg_score += has_links + on_documents\n",
    "        avg_score /= 2.1\n",
    "        thought += f\"\\n-----\\n{resp2['thought']}\"\n",
    "    return {\n",
    "        'reasoning': thought,\n",
    "        'avg_score': avg_score,\n",
    "        'cot_llm': score,\n",
    "        'has_links': has_links,\n",
    "        'on_documents': on_documents\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Проверим работу цепочки оценки ответов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'reasoning': 'EXPLANATION: Чтобы оценить ответ студента, сначала нужно определить, что требуется в вопросе. Вопрос спрашивает о главном герое книги. Далее, мы сравниваем ответ студента с корректным ответом. Корректный ответ - \"Собака\", а студент ответил \"Кот\". Эти два ответа не совпадают. Поскольку ответ студента не соответствует корректному ответу, он является неправильным. В данном случае, ответ студента не содержит дополнительной информации, которая могла бы повлиять на оценку, и не соответствует фактической информации, предоставленной в корректном ответе.\\n\\nGRADE: INCORRECT',\n",
       " 'avg_score': 0,\n",
       " 'cot_llm': 0,\n",
       " 'has_links': 0,\n",
       " 'on_documents': 0}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Тут оценка неправильного ответа от LLM\n",
    "await evaluation(query=\"Кто главный герой книги\", output=\"Кот\", expected_output=\"Собака\", documents=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'reasoning': 'EXPLANATION: Чтобы оценить ответ студента, нужно сравнить его с корректным ответом. В данном случае, корректный ответ - \"Котик\". Ответ студента - \"Кот\". Оба ответа указывают на одно и то же существо, однако в ответе студента используется более обобщённое название. Важно определить, является ли это различие критичным для понимания и точности ответа. В данном контексте, \"Кот\" и \"Котик\" могут быть восприняты как одно и то же, если в книге не делается акцент на уменьшительно-ласкательной форме имени. Поскольку в вопросе не указано, что форма имени имеет значение, можно считать, что ответ студента передаёт ту же информацию, что и правильный ответ.\\n\\nGRADE: CORRECT',\n",
       " 'avg_score': 1,\n",
       " 'cot_llm': 1,\n",
       " 'has_links': 0,\n",
       " 'on_documents': 0}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Тут оценка правильного ответа от LLM\n",
    "await evaluation(\"Кто главный герой книги\", \"Кот\", \"Котик\", [])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Внимание!\n",
    "Возможно скоро COT Chain цепочка устареет и станет deprecated, поэтому здесь [another_cot_chain.ipynb](another_cot_chain.ipynb)\n",
    "вы можете найти более свежий пример данной цепочки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Оценка\n",
    "### Функция оценки\n",
    "\n",
    "Мы будем оценивать качество ответов по следующим параметрам:\n",
    "Корректность ответа (0-1 балл)\n",
    "Основан ли ответ на документах? (0-1 балл)\n",
    "Содержит ли ответ ссылки на документы? (0-0.1 балл)\n",
    "\n",
    "Далее оценка суммируется и нормируется таким образом, чтобы суммарная оценка была от 0 до 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from tqdm.asyncio import tqdm\n",
    "from typing import Any, Coroutine\n",
    "from langchain.schema import BaseMessage\n",
    "from langchain.schema import AIMessage\n",
    "from graph import graph, GraphState\n",
    "\n",
    "\n",
    "async def evaluate(run_name: str, generator: Coroutine[Any, Any, BaseMessage], graph=False):\n",
    "    dataset = langfuse.get_dataset(\"rag_dataset\")\n",
    "\n",
    "    async def without_rag(item, run_name, semaphore, retries=1):\n",
    "        async with semaphore:\n",
    "            for _ in range(retries):\n",
    "                handler = item.get_langchain_handler(run_name=run_name)\n",
    "                try:\n",
    "                    inp = item\n",
    "                    if graph:\n",
    "                        inp = GraphState(question=item.input)\n",
    "                    else:\n",
    "                        inp = item.input\n",
    "                        \n",
    "                    generation = (await generator.ainvoke(input=inp, config={\"callbacks\": [handler]}))\n",
    "                    answer = \"\"\n",
    "                    context = []\n",
    "                    if isinstance(generation, str):\n",
    "                        answer = generation\n",
    "                    elif isinstance(generation, AIMessage):\n",
    "                        answer = generation.content\n",
    "                    else:\n",
    "                        answer = generation.get('answer', generation.get('generation', \"\"))\n",
    "                        context = generation.get('context', generation.get('documents', []))\n",
    "                    resp = await evaluation(input, answer, item.expected_output, context)\n",
    "                    \n",
    "                    handler.trace.score(\n",
    "                        name=\"avg_score\",\n",
    "                        value=resp['avg_score'],\n",
    "                        comment=resp['reasoning']\n",
    "                    )\n",
    "                    for score_name in ['cot_llm', 'has_links', 'on_documents']:\n",
    "                        handler.trace.score(\n",
    "                            name=score_name,\n",
    "                            value=resp[score_name]\n",
    "                        )\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "                    handler.trace.score(\n",
    "                        name=\"avg_score\",\n",
    "                        value=0,\n",
    "                        comment=str(e)\n",
    "                    )\n",
    "                    for score_name in ['cot_llm', 'has_links', 'on_documents']:\n",
    "                        handler.trace.score(\n",
    "                            name=score_name,\n",
    "                            value=0\n",
    "                        )\n",
    "\n",
    "    tasks = []\n",
    "    sem = asyncio.Semaphore(5)\n",
    "\n",
    "    for item in dataset.items:\n",
    "        tasks.append(without_rag(item, run_name, sem))\n",
    "\n",
    "    r = await tqdm.gather(*tasks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Оценка GigaChat lite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Обновление GigaChain может включать несколько шагов в зависимости от версии и типа обновления. Вот общий процесс для обновления GigaChain:\\n\\n1. **Проверка текущей версии**: Убедитесь, что у вас установлена правильная версия GigaChain. Это можно сделать с помощью команды `gchain version`.\\n\\n2. **Подготовка к обновлению**:\\n   - Создайте резервную копию базы данных и всех конфигурационных файлов.\\n   - Остановите все сервисы GigaChain (например, `gserver` и `gwallet`).\\n\\n3. **Скачивание новой версии**: Перейдите на официальный сайт GigaChain и скачайте последнюю версию.\\n\\n4. **Обновление кода**: Распакуйте скачанный архив и замените существующие файлы и директории новыми.\\n\\n5. **Конфигурация**: Если в новой версии изменилась структура конфигурации, настройте её соответствующим образом.\\n\\n6. **Запуск сервисов**: Запустите сервер и кошелек снова (`gserver`, `gwallet`), чтобы убедиться, что всё работает корректно.\\n\\n7. **Тестирование**: Проведите тестирование обновленной системы, чтобы убедиться в её работоспособности.\\n\\nЕсли у вас возникнут какие-либо проблемы или вопросы по ходу процесса, рекомендуется обратиться к документации GigaChain или связаться с поддержкой проекта.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.chat_models import GigaChat\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "giga = GigaChat(model=\"GigaChat\", temperature=1e-15, profanity_check=False, max_tokens=8000)\n",
    "chain_giga_lite = giga | StrOutputParser()\n",
    "\n",
    "chain_giga_lite.invoke(\"Как обновить GigaChain?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████▉  | 59/74 [03:12<00:44,  2.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 66/74 [03:39<00:23,  2.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 74/74 [04:10<00:00,  3.38s/it]\n"
     ]
    }
   ],
   "source": [
    "await evaluate(\"gigachat_lite\", chain_giga_lite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "giga_pro = GigaChat(model=\"GigaChat-Pro\", temperature=0.00001, profanity_check=False, max_tokens=8000)\n",
    "await evaluate(\"gigachat_pro\", giga_pro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0, model=\"gpt-4o-2024-08-06\")\n",
    "await evaluate(\"gpt-4o\", llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь мы можем зайти в интерфейс GigaLogger и увидеть получившиеся оценки - 0.11 для gigachat lite и 0.15 для gigachat-pro."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](media/llm_without_rag.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Судя по всему GigaChat хорошо справляется с вопросами сам о себе, но про GigaChain отвечает слабо.\n",
    "Теперь попробуем прогнать датасет с простым RAG\n",
    "\n",
    "### Оценка ответов GigaChat + RAG(стандартный)\n",
    "Для начала инициализируем векторную базу данных. В этом примере используется внешняя БД Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain_community.embeddings.gigachat import GigaChatEmbeddings\n",
    "from pinecone import Pinecone\n",
    "\n",
    "pinecone_api_key = os.environ.get(\"PINECONE_API_KEY\")\n",
    "\n",
    "pc = Pinecone(api_key=pinecone_api_key)\n",
    "index = pc.Index(\"gigachain-test-index-gigar\")\n",
    "\n",
    "embeddings = GigaChatEmbeddings(model=\"EmbeddingsGigaR\")\n",
    "vector_store = PineconeVectorStore(index=index, embedding=embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Собираем цепочку с RAG\n",
    "### Классический RAG без документов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.chains.question_answering.stuff_prompt import CHAT_PROMPT\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "vector_store.as_retriever().invoke(\"Как обновить GigaChain?\")\n",
    "\n",
    "rag_chain = (\n",
    "    {\n",
    "        \"context\": vector_store.as_retriever() | format_docs,\n",
    "        \"question\": RunnablePassthrough(),\n",
    "    }\n",
    "    | CHAT_PROMPT\n",
    "    | giga_pro\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Выполните команду bash pip install -U gigachain_community'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\"Как обновить GigaChain?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'context': [Document(metadata={'source': 'https://giga.chat/help/articles/faq'}, page_content='Как обновить GigaChain?\\n\\nВыполните команду bash pip install -U gigachain_community\\n\\n## Как установить gigachain\\n\\nВыполните команду bash pip install gigachain_community\\n\\n## Как обновиться с langchain на gigachain Нужно создать чистое виртуальное окружение python и активировать его. Для Mac/Linux, например так: bash python -m venv venv source venv/bin/activate\\n\\nДалее можно установить gigachain: bash pip install gigachain_community\\n\\nРучное удаление langchain не рекомендуется.\\n\\n## Как установить LangGraph bash pip install langgraph'),\n",
       "  Document(metadata={'source': 'https://courses.sberuniversity.ru/llm-gigachat/'}, page_content='4.1 Что такое GigaChain и как его установить\\n\\nВведение\\n\\nGigaChain SDK — это библиотека инструментов для упрощения и автоматизации работы c GigaChat. Информация из этого урока и далее будет полезна, прежде всего, разработчикам, которые занимаются интеграцией GigaChat с продуктами для бизнеса.\\n\\nGigaChain — это версия на базе библиотеки LangChain для работы с русским языком, что позволяет использовать её при работе и с другими языковыми моделями.\\n\\nБиблиотека стандартизирует типовые кейсы использования языковых моделей (работа с цепочками, базами знаний и документами) и содержит набор готовых промптов для решения бизнес-задач.\\n\\nДо этого момента мы показывали взаимодействие с GigaChat API на тестовой платформе Postman. Вообще же системный промптинг — это написание команд с помощью языка программирования. В курсе мы приводим разбор инструментов и задач с иллюстрациями, но, чтобы самостоятельно протестировать методы из урока, вам потребуется войти в среду разработки, например, PyCharm.\\n\\nВ этом уроке вы узнаете, как установить библиотеку GigaChain, работать с промптами и пользоваться хабом готовых промптов.\\n\\nКак установить и пользоваться GigaChain\\n\\nИтак, GigaСhain – это ответвление (fork) открытой библиотеки LangСhain на Python. В библиотеке много различных утилит и компонентов для работы с промптами. Базовый объект GigaChain — цепочки, последовательности вызовов к модели и другим инструментам.\\n\\nВ GigaChat SDK вы найдёте:\\n\\nБиблиотеку, которая содержит интерфейсы и интеграции для разных компонентов, базовую среду выполнения для объединения этих компонентов в цепочки и агенты, готовые реализации цепочек и агентов.\\n\\nКаталог (хаб) промптов. Набор типовых отлаженных промптов для решения различных задач.\\n\\nGigaChain Templates. Это коллекция легко развёртываемых шаблонных решений для широкого спектра задач.\\n\\nGigaServe. Библиотека, позволяющая публиковать цепочки GigaChain в форме REST API.'),\n",
       "  Document(metadata={'source': 'https://github.com/ai-forever/gigachain'}, page_content=\"Подробнее о том, как внести свой вклад.\\n\\n📖 Дополнительная документация\\n\\n[!NOTE] Полная документация GigaChain находится в процессе перевода. Вы можете также пользоваться документацией LangChain, поскольку GigaChain совместим с LangChain:\\n\\nIntroduction: Overview of the framework and the structure of the docs.\\n\\nTutorials: If you're looking to build something specific or are more of a hands-on learner, check out our tutorials. This is the best place to get started.\\n\\nHow-to guides: Answers to “How do I….?” type questions. These guides are goal-oriented and concrete; they're meant to help you complete a specific task.\\n\\nConceptual guide: Conceptual explanations of the key parts of the framework.\\n\\nAPI Reference: Thorough documentation of every class and method.\\n\\nЛицензия\\n\\nПроект распространяется по лицензии MIT, доступной в файле LICENSE.\\n\\n[^1]: В настоящий момент эта функциональность доступна в бета-режиме.\"),\n",
       "  Document(metadata={'source': 'https://github.com/ai-forever/gigachain'}, page_content='🦜️🔗 GigaChain (GigaChat + LangChain)\\n\\nБиблиотека для разработки LangChain-style приложений на русском языке с поддержкой GigaChat Создать issue · Документация GigaChain\\n\\n🤔 Что такое GigaChain?\\n\\nGigaChain это фреймворк для разработки приложений с использованием больших языковых моделей (LLM), таких, как GigaChat или YandexGPT. Он позволяет создавать приложения, которые:\\n\\nУчитывают контекст — подключите свою модель к источникам данных.\\n\\nМогут рассуждать — положитесь на модель в построении рассуждениях (о том, как ответить, опираясь на контекст, какие действия предпринять и т.д.).\\n\\n[!WARNING] Версия библиотеки LangChain адаптированная для русского языка с поддержкой нейросетевой модели GigaChat. Библиотека GigaChain обратно совместима с LangChain, что позволяет использовать ее не только для работы с GigaChat, но и при работе с другими LLM в различных комбинациях.\\n\\nФреймворк включает:\\n\\nБиблиотеку GigaChain. Библиотека на Python содержит интерфейсы и интеграции для множества компонентов, базовую среду выполнения для объединения этих компонентов в цепочки и агенты, а также готовые реализации цепочек и агентов.\\n\\nХаб промптов. Набор типовых отлаженных промптов для решения различных задач.\\n\\nGigaChain Templates. Коллекция легко развертываемых шаблонных решений для широкого спектра задач.\\n\\nGigaServe. Библиотека, позволяющая публиковать цепочки GigaChain в форме REST API.\\n\\nGigaGraph. Библиотека, дающая возможность работать с LLM (большими языковыми моделями), для создания приложений, которые используют множество взаимодействующих цепочек (акторов) и сохраняют данные о состоянии. Так как в основе GigaGraph лежит GigaChain, предполагается совместное использование обеих библиотек.\\n\\nКроме этого, фреймворк совместим со сторонним сервисом LangSmith — платформой для разработчиков, которая позволяет отлаживать, тестировать, оценивать и отслеживать цепочки, построенные на любой платформе LLM, и легко интегрируется с LangChain и GigaChain.\\n\\nРепозиторий содержит следующие компоненты:')],\n",
       " 'question': 'Как обновить GigaChain?',\n",
       " 'answer': 'Выполните команду bash pip install -U gigachain_community'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableParallel\n",
    "\n",
    "rag_chain = (\n",
    "    RunnablePassthrough.assign(context=(lambda x: format_docs(x[\"context\"])))\n",
    "    | CHAT_PROMPT\n",
    "    | giga_pro\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "rag_chain_with_source = RunnableParallel(\n",
    "    {\"context\": vector_store.as_retriever(), \"question\": RunnablePassthrough()}\n",
    ").assign(answer=rag_chain)\n",
    "\n",
    "rag_chain_with_source.invoke(\"Как обновить GigaChain?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 74/74 [06:55<00:00,  5.62s/it]\n"
     ]
    }
   ],
   "source": [
    "await evaluate(\"rag_gigar\", rag_chain_with_source)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "![image-2.png](media/llm_with_rag.jpeg)\n",
    "\n",
    "Оценка - 0.71\n",
    "\n",
    "### Оценка ответов GigaChat + Adaptive RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "CancelledError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCancelledError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgraph\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m graph, GraphState\n\u001b[0;32m----> 3\u001b[0m generation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m graph\u001b[38;5;241m.\u001b[39mainvoke(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39mGraphState(question\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mКак обновить GigaChain?\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(generation[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgeneration\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m~/Documents/giga/gigachain_telegram_bot/venv/lib/python3.12/site-packages/langgraph/pregel/__init__.py:1580\u001b[0m, in \u001b[0;36mPregel.ainvoke\u001b[0;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, **kwargs)\u001b[0m\n\u001b[1;32m   1578\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1579\u001b[0m     chunks \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m-> 1580\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mastream(\n\u001b[1;32m   1581\u001b[0m     \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m   1582\u001b[0m     config,\n\u001b[1;32m   1583\u001b[0m     stream_mode\u001b[38;5;241m=\u001b[39mstream_mode,\n\u001b[1;32m   1584\u001b[0m     output_keys\u001b[38;5;241m=\u001b[39moutput_keys,\n\u001b[1;32m   1585\u001b[0m     interrupt_before\u001b[38;5;241m=\u001b[39minterrupt_before,\n\u001b[1;32m   1586\u001b[0m     interrupt_after\u001b[38;5;241m=\u001b[39minterrupt_after,\n\u001b[1;32m   1587\u001b[0m     debug\u001b[38;5;241m=\u001b[39mdebug,\n\u001b[1;32m   1588\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   1589\u001b[0m ):\n\u001b[1;32m   1590\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stream_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1591\u001b[0m         latest \u001b[38;5;241m=\u001b[39m chunk\n",
      "File \u001b[0;32m~/Documents/giga/gigachain_telegram_bot/venv/lib/python3.12/site-packages/langgraph/pregel/__init__.py:1424\u001b[0m, in \u001b[0;36mPregel.astream\u001b[0;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug)\u001b[0m\n\u001b[1;32m   1422\u001b[0m     done, inflight \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(), \u001b[38;5;28mset\u001b[39m()\n\u001b[1;32m   1423\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m futures:\n\u001b[0;32m-> 1424\u001b[0m     done, inflight \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mwait(\n\u001b[1;32m   1425\u001b[0m         futures,\n\u001b[1;32m   1426\u001b[0m         return_when\u001b[38;5;241m=\u001b[39masyncio\u001b[38;5;241m.\u001b[39mFIRST_COMPLETED,\n\u001b[1;32m   1427\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1428\u001b[0m             \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m0\u001b[39m, end_time \u001b[38;5;241m-\u001b[39m aioloop\u001b[38;5;241m.\u001b[39mtime()) \u001b[38;5;28;01mif\u001b[39;00m end_time \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1429\u001b[0m         ),\n\u001b[1;32m   1430\u001b[0m     )\n\u001b[1;32m   1431\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[1;32m   1432\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m  \u001b[38;5;66;03m# timed out\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.5/lib/python3.12/asyncio/tasks.py:464\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(fs, timeout, return_when)\u001b[0m\n\u001b[1;32m    461\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPassing coroutines is forbidden, use tasks explicitly.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    463\u001b[0m loop \u001b[38;5;241m=\u001b[39m events\u001b[38;5;241m.\u001b[39mget_running_loop()\n\u001b[0;32m--> 464\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m _wait(fs, timeout, return_when, loop)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.5/lib/python3.12/asyncio/tasks.py:550\u001b[0m, in \u001b[0;36m_wait\u001b[0;34m(fs, timeout, return_when, loop)\u001b[0m\n\u001b[1;32m    547\u001b[0m     f\u001b[38;5;241m.\u001b[39madd_done_callback(_on_completion)\n\u001b[1;32m    549\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 550\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m waiter\n\u001b[1;32m    551\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    552\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout_handle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mCancelledError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from graph import graph, GraphState\n",
    "\n",
    "generation = await graph.ainvoke(input=GraphState(question=\"Как обновить GigaChain?\"))\n",
    "print(generation['generation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 40/74 [02:26<01:32,  2.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(URL('https://wmapi-ift.saluteai-pd.sberdevices.ru/v1/chat/completions'), 500, b'{\"status\":500,\"message\":\"Internal Server Error\"}\\n', Headers({'server': 'nginx', 'date': 'Tue, 24 Sep 2024 08:00:32 GMT', 'content-type': 'application/json; charset=utf-8', 'content-length': '49', 'connection': 'keep-alive', 'keep-alive': 'timeout=15', 'access-control-allow-credentials': 'true', 'access-control-allow-headers': 'Origin, X-Requested-With, Content-Type, Accept, Authorization', 'access-control-allow-methods': 'GET, POST, DELETE, OPTIONS', 'access-control-allow-origin': 'https://beta.saluteai.sberdevices.ru', 'x-request-id': '7a9f80a6-c5d8-4d89-b43d-56e891cef00a', 'x-session-id': '7b38898a-c810-4e73-92cc-8f8a6f627bc5', 'x-sp-crid': '1851564027:2'}))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 74/74 [04:09<00:00,  3.37s/it]\n"
     ]
    }
   ],
   "source": [
    "await evaluate(\"arag_gigar\", graph, graph=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Смотрим результат...\n",
    "![image-2.png](media/llm_with_arag.jpeg)\n",
    "Результат вышел `0.59`, меньше чем у просто RAG.\n",
    "Почему?\n",
    "Дело в том, что ARAG сам выбирает относиться ли вопрос к нашей векторной базе данных,\n",
    "и может отказаться от ответа, не обращаясь к ней. Здесь качество зависит от качества промпта\n",
    "который направляет запрос в графе.\n",
    "### Оценка ответов Support Bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Для обновления GigaChain выполните команду `bash pip install -U gigachain_community`. Дополнительную информацию и примеры использования GigaChain можно найти в следующих ресурсах:\\n\\n- Документация по API: [https://developers.sber.ru/docs/ru/gigachat/api/overview](https://developers.sber.ru/docs/ru/gigachat/api/overview)\\n- Репозиторий GigaChain на GitHub: [https://github.com/ai-forever/gigachain](https://github.com/ai-forever/gigachain)\\n- Курс по LLM GigaChat: [https://courses.sberuniversity.ru/llm-gigachat/](https://courses.sberuniversity.ru/llm-gigachat/)\\n\\nЭти ресурсы предоставят подробные руководства и примеры кода для работы с GigaChain.'"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from graph_2 import graph as graph_2\n",
    "(await graph_2.ainvoke(input=GraphState(question=\"Как обновить GigaChain?\")))['generation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 74/74 [16:07<00:00, 13.07s/it]\n"
     ]
    }
   ],
   "source": [
    "await evaluate(\"support_bot_v2_gigar_2\", graph_2, graph=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Смотрим результат...\n",
    "![image.png](media/llm_with_support_bot.jpeg)\n",
    "Результат вышел `0.75`.\n",
    "На данный момент эта версия дает наилучшее качество"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Версия Support RAG v3 - доработанная версия для telegram-бота"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgraph\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m graph, GraphState\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgraph_3\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m graph \u001b[38;5;28;01mas\u001b[39;00m graph_3\n\u001b[0;32m----> 4\u001b[0m (\u001b[38;5;28;01mawait\u001b[39;00m graph_3\u001b[38;5;241m.\u001b[39mainvoke(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39mGraphState(question\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mКак обновить GigaChain?\u001b[39m\u001b[38;5;124m\"\u001b[39m)))[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgeneration\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/Documents/giga/gigachain_telegram_bot/venv/lib/python3.12/site-packages/langgraph/pregel/__init__.py:1580\u001b[0m, in \u001b[0;36mPregel.ainvoke\u001b[0;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, **kwargs)\u001b[0m\n\u001b[1;32m   1578\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1579\u001b[0m     chunks \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m-> 1580\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mastream(\n\u001b[1;32m   1581\u001b[0m     \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m   1582\u001b[0m     config,\n\u001b[1;32m   1583\u001b[0m     stream_mode\u001b[38;5;241m=\u001b[39mstream_mode,\n\u001b[1;32m   1584\u001b[0m     output_keys\u001b[38;5;241m=\u001b[39moutput_keys,\n\u001b[1;32m   1585\u001b[0m     interrupt_before\u001b[38;5;241m=\u001b[39minterrupt_before,\n\u001b[1;32m   1586\u001b[0m     interrupt_after\u001b[38;5;241m=\u001b[39minterrupt_after,\n\u001b[1;32m   1587\u001b[0m     debug\u001b[38;5;241m=\u001b[39mdebug,\n\u001b[1;32m   1588\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   1589\u001b[0m ):\n\u001b[1;32m   1590\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stream_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1591\u001b[0m         latest \u001b[38;5;241m=\u001b[39m chunk\n",
      "File \u001b[0;32m~/Documents/giga/gigachain_telegram_bot/venv/lib/python3.12/site-packages/langgraph/pregel/__init__.py:1463\u001b[0m, in \u001b[0;36mPregel.astream\u001b[0;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug)\u001b[0m\n\u001b[1;32m   1460\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m   1462\u001b[0m \u001b[38;5;66;03m# panic on failure or timeout\u001b[39;00m\n\u001b[0;32m-> 1463\u001b[0m \u001b[43m_panic_or_proceed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_futures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43masyncio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTimeoutError\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1464\u001b[0m \u001b[38;5;66;03m# don't keep futures around in memory longer than needed\u001b[39;00m\n\u001b[1;32m   1465\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m done, inflight, futures\n",
      "File \u001b[0;32m~/Documents/giga/gigachain_telegram_bot/venv/lib/python3.12/site-packages/langgraph/pregel/__init__.py:1643\u001b[0m, in \u001b[0;36m_panic_or_proceed\u001b[0;34m(futs, step, timeout_exc_cls)\u001b[0m\n\u001b[1;32m   1641\u001b[0m             inflight\u001b[38;5;241m.\u001b[39mpop()\u001b[38;5;241m.\u001b[39mcancel()\n\u001b[1;32m   1642\u001b[0m         \u001b[38;5;66;03m# raise the exception\u001b[39;00m\n\u001b[0;32m-> 1643\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[1;32m   1645\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inflight:\n\u001b[1;32m   1646\u001b[0m     \u001b[38;5;66;03m# if we got here means we timed out\u001b[39;00m\n\u001b[1;32m   1647\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m inflight:\n\u001b[1;32m   1648\u001b[0m         \u001b[38;5;66;03m# cancel all pending tasks\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/giga/gigachain_telegram_bot/venv/lib/python3.12/site-packages/langgraph/pregel/retry.py:79\u001b[0m, in \u001b[0;36marun_with_retry\u001b[0;34m(task, retry_policy, stream)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 79\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m task\u001b[38;5;241m.\u001b[39mproc\u001b[38;5;241m.\u001b[39mainvoke(task\u001b[38;5;241m.\u001b[39minput, task\u001b[38;5;241m.\u001b[39mconfig)\n\u001b[1;32m     80\u001b[0m \u001b[38;5;66;03m# if successful, end\u001b[39;00m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/giga/gigachain_telegram_bot/venv/lib/python3.12/site-packages/langchain_core/runnables/base.py:2922\u001b[0m, in \u001b[0;36mRunnableSequence.ainvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   2920\u001b[0m     part \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mpartial(step\u001b[38;5;241m.\u001b[39mainvoke, \u001b[38;5;28minput\u001b[39m, config)\n\u001b[1;32m   2921\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m asyncio_accepts_context():\n\u001b[0;32m-> 2922\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mcreate_task(part(), context\u001b[38;5;241m=\u001b[39mcontext)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m   2923\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2924\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mcreate_task(part())\n",
      "File \u001b[0;32m~/Documents/giga/gigachain_telegram_bot/venv/lib/python3.12/site-packages/langgraph/utils.py:124\u001b[0m, in \u001b[0;36mRunnableCallable.ainvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    122\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m config\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sys\u001b[38;5;241m.\u001b[39mversion_info \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m11\u001b[39m):\n\u001b[0;32m--> 124\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mcreate_task(\n\u001b[1;32m    125\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mafunc(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs), context\u001b[38;5;241m=\u001b[39mcontext\n\u001b[1;32m    126\u001b[0m     )\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    128\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mafunc(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/Documents/giga/gigachain_telegram_bot/evaluation/../graph_3.py:153\u001b[0m, in \u001b[0;36mretrieve\u001b[0;34m(state)\u001b[0m\n\u001b[1;32m    150\u001b[0m question \u001b[38;5;241m=\u001b[39m state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    152\u001b[0m \u001b[38;5;66;03m# Retrieval\u001b[39;00m\n\u001b[0;32m--> 153\u001b[0m documents \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m retriever\u001b[38;5;241m.\u001b[39mainvoke(question)\n\u001b[1;32m    154\u001b[0m retrieve_count \u001b[38;5;241m=\u001b[39m state\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mretrieve_count\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m retrieve_count:\n",
      "File \u001b[0;32m~/Documents/giga/gigachain_telegram_bot/venv/lib/python3.12/site-packages/langchain_core/retrievers.py:314\u001b[0m, in \u001b[0;36mBaseRetriever.ainvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    312\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    313\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m run_manager\u001b[38;5;241m.\u001b[39mon_retriever_error(e)\n\u001b[0;32m--> 314\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    315\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    316\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m run_manager\u001b[38;5;241m.\u001b[39mon_retriever_end(\n\u001b[1;32m    317\u001b[0m         result,\n\u001b[1;32m    318\u001b[0m     )\n",
      "File \u001b[0;32m~/Documents/giga/gigachain_telegram_bot/venv/lib/python3.12/site-packages/langchain_core/retrievers.py:307\u001b[0m, in \u001b[0;36mBaseRetriever.ainvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    305\u001b[0m _kwargs \u001b[38;5;241m=\u001b[39m kwargs \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expects_other_args \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[1;32m    306\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_new_arg_supported:\n\u001b[0;32m--> 307\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_aget_relevant_documents(\n\u001b[1;32m    308\u001b[0m         \u001b[38;5;28minput\u001b[39m, run_manager\u001b[38;5;241m=\u001b[39mrun_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_kwargs\n\u001b[1;32m    309\u001b[0m     )\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    311\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_aget_relevant_documents(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_kwargs)\n",
      "File \u001b[0;32m~/Documents/giga/gigachain_telegram_bot/venv/lib/python3.12/site-packages/langchain_core/vectorstores/base.py:1060\u001b[0m, in \u001b[0;36mVectorStoreRetriever._aget_relevant_documents\u001b[0;34m(self, query, run_manager)\u001b[0m\n\u001b[1;32m   1056\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_aget_relevant_documents\u001b[39m(\n\u001b[1;32m   1057\u001b[0m     \u001b[38;5;28mself\u001b[39m, query: \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;241m*\u001b[39m, run_manager: AsyncCallbackManagerForRetrieverRun\n\u001b[1;32m   1058\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Document]:\n\u001b[1;32m   1059\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msearch_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msimilarity\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 1060\u001b[0m         docs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvectorstore\u001b[38;5;241m.\u001b[39masimilarity_search(\n\u001b[1;32m   1061\u001b[0m             query, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msearch_kwargs\n\u001b[1;32m   1062\u001b[0m         )\n\u001b[1;32m   1063\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msearch_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msimilarity_score_threshold\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1064\u001b[0m         docs_and_similarities \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1065\u001b[0m             \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvectorstore\u001b[38;5;241m.\u001b[39masimilarity_search_with_relevance_scores(\n\u001b[1;32m   1066\u001b[0m                 query, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msearch_kwargs\n\u001b[1;32m   1067\u001b[0m             )\n\u001b[1;32m   1068\u001b[0m         )\n",
      "File \u001b[0;32m~/Documents/giga/gigachain_telegram_bot/venv/lib/python3.12/site-packages/langchain_core/vectorstores/base.py:641\u001b[0m, in \u001b[0;36mVectorStore.asimilarity_search\u001b[0;34m(self, query, k, **kwargs)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Async return docs most similar to query.\u001b[39;00m\n\u001b[1;32m    628\u001b[0m \n\u001b[1;32m    629\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;124;03m    List of Documents most similar to the query.\u001b[39;00m\n\u001b[1;32m    636\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    638\u001b[0m \u001b[38;5;66;03m# This is a temporary workaround to make the similarity search\u001b[39;00m\n\u001b[1;32m    639\u001b[0m \u001b[38;5;66;03m# asynchronous. The proper solution is to make the similarity search\u001b[39;00m\n\u001b[1;32m    640\u001b[0m \u001b[38;5;66;03m# asynchronous in the vector store implementations.\u001b[39;00m\n\u001b[0;32m--> 641\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m run_in_executor(\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msimilarity_search, query, k\u001b[38;5;241m=\u001b[39mk, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/Documents/giga/gigachain_telegram_bot/venv/lib/python3.12/site-packages/langchain_core/runnables/config.py:589\u001b[0m, in \u001b[0;36mrun_in_executor\u001b[0;34m(executor_or_config, func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    585\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n\u001b[1;32m    587\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m executor_or_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(executor_or_config, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    588\u001b[0m     \u001b[38;5;66;03m# Use default executor with context copied from current context\u001b[39;00m\n\u001b[0;32m--> 589\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mget_running_loop()\u001b[38;5;241m.\u001b[39mrun_in_executor(\n\u001b[1;32m    590\u001b[0m         \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    591\u001b[0m         cast(Callable[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, T], partial(copy_context()\u001b[38;5;241m.\u001b[39mrun, wrapper)),\n\u001b[1;32m    592\u001b[0m     )\n\u001b[1;32m    594\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mget_running_loop()\u001b[38;5;241m.\u001b[39mrun_in_executor(executor_or_config, wrapper)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.5/lib/python3.12/concurrent/futures/thread.py:58\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuture\u001b[38;5;241m.\u001b[39mset_exception(exc)\n",
      "File \u001b[0;32m~/Documents/giga/gigachain_telegram_bot/venv/lib/python3.12/site-packages/langchain_core/runnables/config.py:580\u001b[0m, in \u001b[0;36mrun_in_executor.<locals>.wrapper\u001b[0;34m()\u001b[0m\n\u001b[1;32m    578\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m    579\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 580\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    581\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    582\u001b[0m         \u001b[38;5;66;03m# StopIteration can't be set on an asyncio.Future\u001b[39;00m\n\u001b[1;32m    583\u001b[0m         \u001b[38;5;66;03m# it raises a TypeError and leaves the Future pending forever\u001b[39;00m\n\u001b[1;32m    584\u001b[0m         \u001b[38;5;66;03m# so we need to convert it to a RuntimeError\u001b[39;00m\n\u001b[1;32m    585\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/giga/gigachain_telegram_bot/venv/lib/python3.12/site-packages/langchain_pinecone/vectorstores.py:259\u001b[0m, in \u001b[0;36mPineconeVectorStore.similarity_search\u001b[0;34m(self, query, k, filter, namespace, **kwargs)\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msimilarity_search\u001b[39m(\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    242\u001b[0m     query: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    247\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Document]:\n\u001b[1;32m    248\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return pinecone documents most similar to query.\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \n\u001b[1;32m    250\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[38;5;124;03m        List of Documents most similar to the query and score for each\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 259\u001b[0m     docs_and_scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msimilarity_search_with_score\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mfilter\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mfilter\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnamespace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnamespace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    262\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [doc \u001b[38;5;28;01mfor\u001b[39;00m doc, _ \u001b[38;5;129;01min\u001b[39;00m docs_and_scores]\n",
      "File \u001b[0;32m~/Documents/giga/gigachain_telegram_bot/venv/lib/python3.12/site-packages/langchain_pinecone/vectorstores.py:205\u001b[0m, in \u001b[0;36mPineconeVectorStore.similarity_search_with_score\u001b[0;34m(self, query, k, filter, namespace)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msimilarity_search_with_score\u001b[39m(\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    188\u001b[0m     query: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    191\u001b[0m     namespace: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    192\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Tuple[Document, \u001b[38;5;28mfloat\u001b[39m]]:\n\u001b[1;32m    193\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return pinecone documents most similar to query, along with scores.\u001b[39;00m\n\u001b[1;32m    194\u001b[0m \n\u001b[1;32m    195\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;124;03m        List of Documents most similar to the query and score for each\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msimilarity_search_by_vector_with_score(\n\u001b[0;32m--> 205\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_embedding\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m, k\u001b[38;5;241m=\u001b[39mk, \u001b[38;5;28mfilter\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mfilter\u001b[39m, namespace\u001b[38;5;241m=\u001b[39mnamespace\n\u001b[1;32m    206\u001b[0m     )\n",
      "File \u001b[0;32m~/Documents/giga/gigachain_telegram_bot/venv/lib/python3.12/site-packages/langchain_community/embeddings/gigachat.py:190\u001b[0m, in \u001b[0;36mGigaChatEmbeddings.embed_query\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_prefix_query:\n\u001b[1;32m    189\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprefix_query \u001b[38;5;241m+\u001b[39m text\n\u001b[0;32m--> 190\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/Documents/giga/gigachain_telegram_bot/venv/lib/python3.12/site-packages/langchain_community/embeddings/gigachat.py:136\u001b[0m, in \u001b[0;36mGigaChatEmbeddings.embed_documents\u001b[0;34m(self, texts)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;66;03m# Call for last iteration\u001b[39;00m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m local_texts:\n\u001b[0;32m--> 136\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m embedding \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtexts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_texts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43membed_kwargs\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdata:\n\u001b[1;32m    139\u001b[0m         result\u001b[38;5;241m.\u001b[39mappend(embedding\u001b[38;5;241m.\u001b[39membedding)\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/Documents/giga/gigachain_telegram_bot/venv/lib/python3.12/site-packages/gigachat/client.py:250\u001b[0m, in \u001b[0;36mGigaChatSyncClient.embeddings\u001b[0;34m(self, texts, model)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21membeddings\u001b[39m(\u001b[38;5;28mself\u001b[39m, texts: List[\u001b[38;5;28mstr\u001b[39m], model: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEmbeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Embeddings:\n\u001b[1;32m    249\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Возвращает эмбеддинги\"\"\"\u001b[39;00m\n\u001b[0;32m--> 250\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_decorator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpost_embeddings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msync\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccess_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/giga/gigachain_telegram_bot/venv/lib/python3.12/site-packages/gigachat/client.py:233\u001b[0m, in \u001b[0;36mGigaChatSyncClient._decorator\u001b[0;34m(self, call)\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_validity_token():\n\u001b[1;32m    232\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 233\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    234\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m AuthenticationError:\n\u001b[1;32m    235\u001b[0m         _logger\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAUTHENTICATION ERROR\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/giga/gigachain_telegram_bot/venv/lib/python3.12/site-packages/gigachat/client.py:251\u001b[0m, in \u001b[0;36mGigaChatSyncClient.embeddings.<locals>.<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21membeddings\u001b[39m(\u001b[38;5;28mself\u001b[39m, texts: List[\u001b[38;5;28mstr\u001b[39m], model: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEmbeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Embeddings:\n\u001b[1;32m    249\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Возвращает эмбеддинги\"\"\"\u001b[39;00m\n\u001b[1;32m    250\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decorator(\n\u001b[0;32m--> 251\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[43mpost_embeddings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msync\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccess_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    252\u001b[0m     )\n",
      "File \u001b[0;32m~/Documents/giga/gigachain_telegram_bot/venv/lib/python3.12/site-packages/gigachat/api/post_embeddings.py:45\u001b[0m, in \u001b[0;36msync\u001b[0;34m(client, input_, model, access_token)\u001b[0m\n\u001b[1;32m     43\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m _get_kwargs(input_\u001b[38;5;241m=\u001b[39minput_, model\u001b[38;5;241m=\u001b[39mmodel, access_token\u001b[38;5;241m=\u001b[39maccess_token)\n\u001b[1;32m     44\u001b[0m response \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mrequest(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 45\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_build_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/giga/gigachain_telegram_bot/venv/lib/python3.12/site-packages/gigachat/api/post_embeddings.py:29\u001b[0m, in \u001b[0;36m_build_response\u001b[0;34m(response)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_build_response\u001b[39m(response: httpx\u001b[38;5;241m.\u001b[39mResponse) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Embeddings:\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m HTTPStatus\u001b[38;5;241m.\u001b[39mOK:\n\u001b[0;32m---> 29\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m Embeddings(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m HTTPStatus\u001b[38;5;241m.\u001b[39mUNAUTHORIZED:\n\u001b[1;32m     31\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m AuthenticationError(response\u001b[38;5;241m.\u001b[39murl, response\u001b[38;5;241m.\u001b[39mstatus_code, response\u001b[38;5;241m.\u001b[39mcontent, response\u001b[38;5;241m.\u001b[39mheaders)\n",
      "File \u001b[0;32m~/Documents/giga/gigachain_telegram_bot/venv/lib/python3.12/site-packages/httpx/_models.py:766\u001b[0m, in \u001b[0;36mResponse.json\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    765\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mjson\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: typing\u001b[38;5;241m.\u001b[39mAny) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m typing\u001b[38;5;241m.\u001b[39mAny:\n\u001b[0;32m--> 766\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mjsonlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.5/lib/python3.12/json/__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    341\u001b[0m     s \u001b[38;5;241m=\u001b[39m s\u001b[38;5;241m.\u001b[39mdecode(detect_encoding(s), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msurrogatepass\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[0;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m JSONDecoder\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.5/lib/python3.12/json/decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, s, _w\u001b[38;5;241m=\u001b[39mWHITESPACE\u001b[38;5;241m.\u001b[39mmatch):\n\u001b[1;32m    333\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;124;03m    containing a JSON document).\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \n\u001b[1;32m    336\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 337\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    338\u001b[0m     end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n\u001b[1;32m    339\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m end \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(s):\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.5/lib/python3.12/json/decoder.py:355\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    353\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscan_once(s, idx)\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m--> 355\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpecting value\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, err\u001b[38;5;241m.\u001b[39mvalue) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj, end\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ],
   "source": [
    "from graph import graph, GraphState\n",
    "\n",
    "from graph_3 import graph as graph_3\n",
    "(await graph_3.ainvoke(input=GraphState(question=\"Как обновить GigaChain?\")))['generation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 17/74 [01:27<04:02,  4.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "string indices must be integers, not 'str'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|██▉       | 22/74 [01:44<04:06,  4.73s/it]\n"
     ]
    },
    {
     "ename": "CancelledError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCancelledError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m evaluate(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msupport_bot_v3_gigachat_max_openai_embeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m, graph_3, graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[10], line 64\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(run_name, generator, graph)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m dataset\u001b[38;5;241m.\u001b[39mitems:\n\u001b[1;32m     62\u001b[0m     tasks\u001b[38;5;241m.\u001b[39mappend(without_rag(item, run_name, sem))\n\u001b[0;32m---> 64\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m tqdm\u001b[38;5;241m.\u001b[39mgather(\u001b[38;5;241m*\u001b[39mtasks)\n",
      "File \u001b[0;32m~/Documents/giga/gigachain_telegram_bot/venv/lib/python3.12/site-packages/tqdm/asyncio.py:79\u001b[0m, in \u001b[0;36mtqdm_asyncio.gather\u001b[0;34m(cls, loop, timeout, total, *fs, **tqdm_kwargs)\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m i, \u001b[38;5;28;01mawait\u001b[39;00m f\n\u001b[1;32m     78\u001b[0m ifs \u001b[38;5;241m=\u001b[39m [wrap_awaitable(i, f) \u001b[38;5;28;01mfor\u001b[39;00m i, f \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(fs)]\n\u001b[0;32m---> 79\u001b[0m res \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mawait\u001b[39;00m f \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mas_completed(ifs, loop\u001b[38;5;241m=\u001b[39mloop, timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[1;32m     80\u001b[0m                                          total\u001b[38;5;241m=\u001b[39mtotal, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtqdm_kwargs)]\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [i \u001b[38;5;28;01mfor\u001b[39;00m _, i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(res)]\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.5/lib/python3.12/asyncio/tasks.py:627\u001b[0m, in \u001b[0;36mas_completed.<locals>._wait_for_one\u001b[0;34m()\u001b[0m\n\u001b[1;32m    626\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_wait_for_one\u001b[39m():\n\u001b[0;32m--> 627\u001b[0m     f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m done\u001b[38;5;241m.\u001b[39mget()\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m f \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m         \u001b[38;5;66;03m# Dummy value from _on_timeout().\u001b[39;00m\n\u001b[1;32m    630\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTimeoutError\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.5/lib/python3.12/asyncio/queues.py:158\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getters\u001b[38;5;241m.\u001b[39mappend(getter)\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 158\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m getter\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m    160\u001b[0m     getter\u001b[38;5;241m.\u001b[39mcancel()  \u001b[38;5;66;03m# Just in case getter is not done yet.\u001b[39;00m\n",
      "\u001b[0;31mCancelledError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "await evaluate(\"support_bot_v3_gpt4_openai_embeddings\", graph_3, graph=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
