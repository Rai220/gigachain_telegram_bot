{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Evaluation в GigaLogger\n",
    "В этом ноутбуке мы произведем оценку нашего RAG'а с помощью датасета и мощной LLM (gpt-4o)\n",
    "И не только! Мы также замерим качество ответов на обычном GigaChat (без RAG), с обычным RAG, и Adaptive RAG.\n",
    "У нас в боте используется Adaptive RAG.\n",
    "Предыдущие шаги:\n",
    "1. [Генерация синтетического датасета](1_generate_dataset.ipynb)\n",
    "2. [Загрузка датасета в GigaLogger](2_gigalogger_create_dataset.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import getpass\n",
    "\n",
    "def get_env_var(var_name):\n",
    "    if var_name in os.environ:\n",
    "        return os.environ[var_name]\n",
    "    else:\n",
    "        return getpass.getpass(f\"Enter {var_name}: \")\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")  # Add the parent folder to the sys.path\n",
    "\n",
    "load_dotenv(find_dotenv())\n",
    "os.environ[\"LANGFUSE_HOST\"] = \"https://gigalogger.demo.sberdevices.ru\"\n",
    "os.environ[\"LANGFUSE_PUBLIC_KEY\"] = get_env_var(\"LANGFUSE_PUBLIC_KEY\")\n",
    "os.environ[\"LANGFUSE_SECRET_KEY\"] = get_env_var(\"LANGFUSE_SECRET_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from langfuse import Langfuse\n",
    "langfuse = Langfuse(timeout=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Цепочка для ответов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Определим промпт для цепочки, которая будет сравнивать наши ответы с правильными ответами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "COT_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"query\", \"context\", \"result\"], template=\"\"\"Ты учитель, оценивающий тест.\n",
    "Тебе дан вопрос, контекст, к которому относится вопрос, и ответ студента. Тебе нужно оценить ответ студента как ПРАВИЛЬНЫЙ или НЕПРАВИЛЬНЫЙ, основываясь на контексте.\n",
    "Опиши пошагово своё рассуждение, чтобы убедиться, что твой вывод правильный. Избегай просто указывать правильный ответ с самого начала.\n",
    "\n",
    "Пример формата:\n",
    "QUESTION: здесь вопрос\n",
    "CONTEXT: здесь контекст, к которому относится вопрос\n",
    "STUDENT ANSWER: здесь ответ студента\n",
    "EXPLANATION: пошаговое рассуждение здесь\n",
    "GRADE: CORRECT или INCORRECT здесь\n",
    "\n",
    "Оценивай ответы студента ТОЛЬКО на основе их фактической точности. Игнорируй различия в пунктуации и формулировках между ответом студента и правильным ответом. Ответ студента может содержать больше информации, чем правильный ответ, если в нём нет противоречивых утверждений. Начнём!\n",
    "\n",
    "QUESTION: \"{query}\"\n",
    "CONTEXT: \"{context}\"\n",
    "STUDENT ANSWER: \"{result}\"\n",
    "EXPLANATION:\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from langchain.evaluation import CotQAEvalChain\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.chat_models.gigachat import GigaChat\n",
    "\n",
    "def cot_llm(query, output, expected_output):\n",
    "    # Используйте мощную модель для лучшего сравнения ответов\n",
    "    eval_llm = ChatOpenAI(temperature=0, model=\"gpt-4o\")\n",
    "    # eval_llm = GigaChat(model=\"GigaChat-Pro\", profanity_check=False,)\n",
    "    eval_chain = CotQAEvalChain.from_llm(llm=eval_llm, prompt=COT_PROMPT)\n",
    "    resp = eval_chain.invoke({\n",
    "        \"query\": query, \"context\": expected_output, \"result\": output\n",
    "    })\n",
    "    return eval_chain._prepare_output(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Проверим работу цепочки оценки ответов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'reasoning': 'EXPLANATION:\\n1. Вопрос спрашивает о главном герое книги.\\n2. Контекст указывает, что главным героем является \"Собака\".\\n3. Ответ студента утверждает, что главным героем является \"Кот\".\\n4. Ответ студента не совпадает с контекстом, который указывает на \"Собаку\" как главного героя.\\n\\nGRADE: INCORRECT',\n",
       " 'value': 'INCORRECT',\n",
       " 'score': 0}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Тут оценка неправильного ответа от LLM\n",
    "cot_llm(\"Кто главный герой книги\", \"Кот\", \"Собака\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'reasoning': 'EXPLANATION:\\n1. Прочитаем вопрос: \"Кто главный герой книги\".\\n2. Прочитаем контекст: \"Котик\".\\n3. Прочитаем ответ студента: \"Кот\".\\n4. Сравним ответ студента с контекстом. В контексте указано, что главный герой книги — \"Котик\". Ответ студента — \"Кот\".\\n5. Рассмотрим, является ли \"Кот\" синонимом или сокращением от \"Котик\". В данном случае, \"Кот\" и \"Котик\" могут рассматриваться как одно и то же животное, просто в разных формах (уменьшительно-ласкательная форма \"Котик\" и обычная форма \"Кот\").\\n6. Убедимся, что ответ студента не содержит противоречивых утверждений и соответствует контексту.\\n\\nGRADE: CORRECT',\n",
       " 'value': 'CORRECT',\n",
       " 'score': 1}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Тут оценка правильного ответа от LLM\n",
    "cot_llm(\"Кто главный герой книги\", \"Кот\", \"Котик\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Оценка\n",
    "### Оценка ответов с обычным GigaChat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from langchain_community.chat_models import GigaChat\n",
    "llm = GigaChat(model=\"GigaChat-Pro\", profanity_check=False, timeout=6000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "dataset = langfuse.get_dataset(\"rag_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35/35 [06:56<00:00, 11.89s/it]\n"
     ]
    }
   ],
   "source": [
    "for item in tqdm.tqdm(dataset.items):\n",
    "    handler = item.get_langchain_handler(run_name=\"llm_without_rag_12\")\n",
    "    try:\n",
    "        generation = llm.invoke(input=item.input, config={\"callbacks\": [handler]}).content\n",
    "        resp = cot_llm(item.input, generation, item.expected_output)\n",
    "        handler.trace.score(\n",
    "            name=\"cot_llm\",\n",
    "            value=resp['score'],\n",
    "            comment=resp['reasoning']\n",
    "        )\n",
    "    except Exception as e:\n",
    "        handler.trace.score(\n",
    "            name=\"cot_llm\",\n",
    "            value=0,\n",
    "            comment=str(e)\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Первый прогон сделан. Смотрим результат...\n",
    "![скриншот прогона](media/llm_without_rag.png)\n",
    "Результат вышел `0.23`.\n",
    "Судя по всему GigaChat хорошо справляется с вопросами сам о себе, но про GigaChain отвечает слабо.\n",
    "Теперь попробуем прогнать датасет с простым RAG\n",
    "### Оценка ответов GigaChat + RAG(стандартный)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from graph import vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(llm, retriever=vector_store.as_retriever())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'Как обновить GigaChain?',\n",
       " 'result': 'Выполните команду bash pip install -U gigachain_community'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_chain.invoke(\"Как обновить GigaChain?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35/35 [04:53<00:00,  8.38s/it]\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "dataset = langfuse.get_dataset(\"rag_dataset\")\n",
    "\n",
    "for item in tqdm.tqdm(dataset.items):\n",
    "    handler = item.get_langchain_handler(run_name=\"llm_with_rag\")\n",
    "    try:\n",
    "        generation = qa_chain.invoke(input=item.input, config={\"callbacks\": [handler]})['result']\n",
    "        resp = cot_llm(item.input, generation, item.expected_output)\n",
    "        handler.trace.score(\n",
    "            name=\"cot_llm\",\n",
    "            value=resp['score'],\n",
    "            comment=resp['reasoning']\n",
    "        )\n",
    "    except Exception as e:\n",
    "        handler.trace.score(\n",
    "            name=\"cot_llm\",\n",
    "            value=0,\n",
    "            comment=str(e)\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Смотрим результат...\n",
    "![скриншот прогона](media/llm_with_rag.png)\n",
    "Результат вышел `0.51`.\n",
    "### Оценка ответов GigaChat + Adaptive RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from graph import graph, GraphState"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Команда `bash pip install -U gigachain_community` поможет вам обновить GigaChain до последней версии. Если у вас возникнут вопросы или проблемы при установке, пожалуйста, обратитесь за дополнительной поддержкой.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.invoke(input=GraphState(question=\"Как обновить GigaChain?\"))['generation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▊   | 24/35 [07:01<02:47, 15.19s/it]Giga generation stopped with reason: blacklist\n",
      " 86%|████████▌ | 30/35 [08:13<01:00, 12.15s/it]Received 413 error by Langfuse server, not retrying: <html>\r\n",
      "<head><title>413 Request Entity Too Large</title></head>\r\n",
      "<body bgcolor=\"white\">\r\n",
      "<center><h1>413 Request Entity Too Large</h1></center>\r\n",
      "<hr><center>nginx/1.14.1</center>\r\n",
      "</body>\r\n",
      "</html>\r\n",
      "\n",
      "100%|██████████| 35/35 [09:38<00:00, 16.53s/it]\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "import logging\n",
    "dataset = langfuse.get_dataset(\"rag_dataset\")\n",
    "\n",
    "for item in tqdm.tqdm(dataset.items):\n",
    "    handler = item.get_langchain_handler(run_name=\"llm_with_arag\")\n",
    "    try:\n",
    "        s = GraphState(question=item.input)\n",
    "        generation = graph.invoke(input=s, config={\"callbacks\": [handler]})['generation']\n",
    "        resp = cot_llm(item.input, generation, item.expected_output)\n",
    "        handler.trace.score(\n",
    "            name=\"cot_llm\",\n",
    "            value=resp['score'],\n",
    "            comment=resp['reasoning']\n",
    "        )\n",
    "    except Exception as e:\n",
    "        logging.error(e, exc_info=True)\n",
    "        handler.trace.score(\n",
    "            name=\"cot_llm\",\n",
    "            value=0,\n",
    "            comment=str(e)\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Смотрим результат...\n",
    "![скриншот прогона](media/llm_with_arag.png)\n",
    "Результат вышел `0.57`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/19563044/Documents/giga/gigachain_telegram_bot/venv/lib/python3.12/site-packages/pinecone/data/index.py:1: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "from graph_2 import graph, GraphState"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▊   | 24/35 [05:55<03:06, 16.94s/it]WARNING:langchain_community.chat_models.gigachat:Giga generation stopped with reason: blacklist\n",
      "100%|██████████| 35/35 [08:30<00:00, 14.59s/it]\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "import logging\n",
    "dataset = langfuse.get_dataset(\"rag_dataset\")\n",
    "\n",
    "for item in tqdm.tqdm(dataset.items):\n",
    "    handler = item.get_langchain_handler(run_name=\"llm_with_arag_18\")\n",
    "    try:\n",
    "        s = GraphState(question=item.input)\n",
    "        # generation = graph.invoke(input=s, config={\"callbacks\": [handler]})['generation']\n",
    "        generation = graph.invoke(input=s)['generation']\n",
    "        resp = cot_llm(item.input, generation, item.expected_output)\n",
    "        handler.trace.score(\n",
    "            name=\"cot_llm\",\n",
    "            value=resp['score'],\n",
    "            comment=resp['reasoning']\n",
    "        )\n",
    "    except Exception as e:\n",
    "        logging.error(e, exc_info=True)\n",
    "        handler.trace.score(\n",
    "            name=\"cot_llm\",\n",
    "            value=0,\n",
    "            comment=str(e)\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
