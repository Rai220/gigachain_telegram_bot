{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Evaluation в GigaLogger\n",
    "В этом ноутбуке мы произведем оценку нашего RAG'а с помощью датасета и мощной LLM (gpt-4o)\n",
    "И не только! Мы также замерим качество ответов на обычном GigaChat (без RAG) и замерим качество с обычным RAG.\n",
    "У нас же в боте используется Adaptive RAG."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"LANGFUSE_HOST\"] = \"https://gigalogger.demo.sberdevices.ru\"\n",
    "os.environ[\"LANGFUSE_PUBLIC_KEY\"] = \"...\"\n",
    "os.environ[\"LANGFUSE_SECRET_KEY\"] = \"...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from dotenv import find_dotenv, load_dotenv\n",
    "load_dotenv(find_dotenv())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "from langfuse import Langfuse\n",
    "langfuse = Langfuse()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Цепочка для ответов"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Определим промпт для цепочки, которая будет сравнивать наши ответы с правильными ответами"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "COT_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"query\", \"context\", \"result\"], template=\"\"\"Ты учитель, оценивающий тест.\n",
    "Тебе дан вопрос, контекст, к которому относится вопрос, и ответ студента. Тебе нужно оценить ответ студента как ПРАВИЛЬНЫЙ или НЕПРАВИЛЬНЫЙ, основываясь на контексте.\n",
    "Опиши пошагово своё рассуждение, чтобы убедиться, что твой вывод правильный. Избегай просто указывать правильный ответ с самого начала.\n",
    "\n",
    "Пример формата:\n",
    "QUESTION: здесь вопрос\n",
    "CONTEXT: здесь контекст, к которому относится вопрос\n",
    "STUDENT ANSWER: здесь ответ студента\n",
    "EXPLANATION: пошаговое рассуждение здесь\n",
    "GRADE: CORRECT или INCORRECT здесь\n",
    "\n",
    "Оценивай ответы студента ТОЛЬКО на основе их фактической точности. Игнорируй различия в пунктуации и формулировках между ответом студента и правильным ответом. Ответ студента может содержать больше информации, чем правильный ответ, если в нём нет противоречивых утверждений. Начнём!\n",
    "\n",
    "QUESTION: \"{query}\"\n",
    "CONTEXT: \"{context}\"\n",
    "STUDENT ANSWER: \"{result}\"\n",
    "EXPLANATION:\"\"\"\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "from langchain.evaluation import CotQAEvalChain\n",
    "from langchain_openai import ChatOpenAI\n",
    "def cot_llm(query, output, expected_output):\n",
    "    eval_llm = ChatOpenAI(temperature=0)\n",
    "    eval_chain = CotQAEvalChain.from_llm(llm=eval_llm, prompt=COT_PROMPT)\n",
    "    resp = eval_chain.invoke({\n",
    "        \"query\": query, \"context\": expected_output, \"result\": output\n",
    "    })\n",
    "    return eval_chain._prepare_output(resp)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Проверим работу цепочки оценки ответов"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "{'reasoning': 'Главный герой книги, контекст которой \"Собака\", скорее всего будет собакой, а не котом. Поэтому ответ студента \"Кот\" неверный.\\nGRADE: INCORRECT',\n 'value': 'INCORRECT',\n 'score': 0}"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Тут оценка неправильного ответа от LLM\n",
    "cot_llm(\"Кто главный герой книги\", \"Кот\", \"Собака\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "{'reasoning': 'Студент ответил \"Кот\", что является правильным ответом, так как главный герой книги действительно является котом. Нет противоречий в ответе студента, поэтому он оценивается как ПРАВИЛЬНЫЙ.\\nGRADE: CORRECT',\n 'value': 'CORRECT',\n 'score': 1}"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Тут оценка правильного ответа от LLM\n",
    "cot_llm(\"Кто главный герой книги\", \"Кот\", \"Котик\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Оценка\n",
    "### Оценка ответов с обычным GigaChat"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "from langchain_community.chat_models import GigaChat\n",
    "llm = GigaChat(model=\"GigaChat-Pro\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 16/35 [03:07<03:58, 12.54s/it]Giga generation stopped with reason: blacklist\n",
      "100%|██████████| 35/35 [06:50<00:00, 11.73s/it]\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "dataset = langfuse.get_dataset(\"rag_dataset\")\n",
    "\n",
    "for item in tqdm.tqdm(dataset.items):\n",
    "    handler = item.get_langchain_handler(run_name=\"llm_without_rag_2\")\n",
    "    try:\n",
    "        generation = llm.invoke(input=item.input, config={\"callbacks\": [handler]}).content\n",
    "        resp = cot_llm(item.input, generation, item.expected_output)\n",
    "        handler.trace.score(\n",
    "            name=\"cot_llm\",\n",
    "            value=resp['score'],\n",
    "            comment=resp['reasoning']\n",
    "        )\n",
    "    except Exception as e:\n",
    "        handler.trace.score(\n",
    "            name=\"cot_llm\",\n",
    "            value=0,\n",
    "            comment=str(e)\n",
    "        )"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Первый прогон сделан. Смотрим результат...\n",
    "![image.png](media/llm_without_rag_2.png)\n",
    "Результат вышел `0.42`.\n",
    "Судя по всему GigaChat хорошо справляется с вопросами сам о себе, но про GigaChain отвечает слабо.\n",
    "Теперь попробуем прогнать датасет с простым RAG\n",
    "### Оценка ответов GigaChat + RAG(стандартный)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "from graph import vector_store\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(llm, retriever=vector_store.as_retriever())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "{'query': 'Как обновить GigaChain?',\n 'result': 'Для обновления GigaChain выполните команду `bash pip install -U gigachain_community`.'}"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_chain.invoke(\"Как обновить GigaChain?\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35/35 [04:59<00:00,  8.56s/it]\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "dataset = langfuse.get_dataset(\"rag_dataset\")\n",
    "\n",
    "for item in tqdm.tqdm(dataset.items):\n",
    "    handler = item.get_langchain_handler(run_name=\"llm_with_rag_2\")\n",
    "    try:\n",
    "        generation = qa_chain.invoke(input=item.input, config={\"callbacks\": [handler]})['result']\n",
    "        resp = cot_llm(item.input, generation, item.expected_output)\n",
    "        handler.trace.score(\n",
    "            name=\"cot_llm\",\n",
    "            value=resp['score'],\n",
    "            comment=resp['reasoning']\n",
    "        )\n",
    "    except Exception as e:\n",
    "        handler.trace.score(\n",
    "            name=\"cot_llm\",\n",
    "            value=0,\n",
    "            comment=str(e)\n",
    "        )"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Смотрим результат...\n",
    "![image.png](media/llm_with_rag.png)\n",
    "Результат вышел `0.68`.\n",
    "### Оценка ответов GigaChat + Adaptive RAG"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "from graph import graph, GraphState"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "'Улучшенный ответ:\\n\\nЧтобы обновить GigaChain до последней версии, запустите следующую команду в терминале: `bash pip install -U gigachain_community`. Это позволит вам использовать все последние улучшения и исправления ошибок.'"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.invoke(input=GraphState(question=\"Как обновить GigaChain?\"))['generation']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35/35 [09:29<00:00, 16.28s/it]\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "dataset = langfuse.get_dataset(\"rag_dataset\")\n",
    "\n",
    "for item in tqdm.tqdm(dataset.items):\n",
    "    handler = item.get_langchain_handler(run_name=\"llm_with_arag\")\n",
    "    try:\n",
    "        s = GraphState(question=item.input)\n",
    "        generation = graph.invoke(input=s, config={\"callbacks\": [handler]})['generation']\n",
    "        resp = cot_llm(item.input, generation, item.expected_output)\n",
    "        handler.trace.score(\n",
    "            name=\"cot_llm\",\n",
    "            value=resp['score'],\n",
    "            comment=resp['reasoning']\n",
    "        )\n",
    "    except Exception as e:\n",
    "        handler.trace.score(\n",
    "            name=\"cot_llm\",\n",
    "            value=0,\n",
    "            comment=str(e)\n",
    "        )"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Смотрим результат...\n",
    "![image.png](media/llm_with_arag.png)\n",
    "Результат вышел `0.74`."
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
