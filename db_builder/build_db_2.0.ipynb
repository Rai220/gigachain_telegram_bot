{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Загрузка напрямую из документации GigaChat API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Загрузка документации GigaChat API с developers.sber.ru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import Optional\n",
    "from bs4 import BeautifulSoup, SoupStrainer, Tag\n",
    "\n",
    "from langchain_community.document_loaders.sitemap import SitemapLoader\n",
    "from typing import Generator\n",
    "from bs4.element import Doctype, NavigableString\n",
    "\n",
    "def langchain_docs_extractor(soup: BeautifulSoup) -> str:\n",
    "    # Remove all the tags that are not meaningful for the extraction.\n",
    "    SCAPE_TAGS = [\"nav\", \"footer\", \"aside\", \"script\", \"style\"]\n",
    "    [tag.decompose() for tag in soup.find_all(SCAPE_TAGS)]\n",
    "\n",
    "    def get_text(tag: Tag) -> Generator[str, None, None]:\n",
    "        for child in tag.children:\n",
    "            if isinstance(child, Doctype):\n",
    "                continue\n",
    "\n",
    "            if isinstance(child, NavigableString):\n",
    "                yield child\n",
    "            elif isinstance(child, Tag):\n",
    "                if child.name in [\"h1\", \"h2\", \"h3\", \"h4\", \"h5\", \"h6\"]:\n",
    "                    yield f\"{'#' * int(child.name[1:])} {child.get_text()}\\n\\n\"\n",
    "                elif child.name == \"a\":\n",
    "                    yield f\"[{child.get_text(strip=False)}]({child.get('href')})\"\n",
    "                elif child.name == \"img\":\n",
    "                    yield f\"![{child.get('alt', '')}]({child.get('src')})\"\n",
    "                elif child.name in [\"strong\", \"b\"]:\n",
    "                    yield f\"**{child.get_text(strip=False)}**\"\n",
    "                elif child.name in [\"em\", \"i\"]:\n",
    "                    yield f\"_{child.get_text(strip=False)}_\"\n",
    "                elif child.name == \"br\":\n",
    "                    yield \"\\n\"\n",
    "                elif child.name == \"code\":\n",
    "                    parent = child.find_parent()\n",
    "                    if parent is not None and parent.name == \"pre\":\n",
    "                        classes = parent.attrs.get(\"class\", \"\")\n",
    "\n",
    "                        language = next(\n",
    "                            filter(lambda x: re.match(r\"language-\\w+\", x), classes),\n",
    "                            None,\n",
    "                        )\n",
    "                        if language is None:\n",
    "                            language = \"\"\n",
    "                        else:\n",
    "                            language = language.split(\"-\")[1]\n",
    "\n",
    "                        lines: list[str] = []\n",
    "                        for span in child.find_all(\"span\", class_=\"token-line\"):\n",
    "                            line_content = \"\".join(\n",
    "                                token.get_text() for token in span.find_all(\"span\")\n",
    "                            )\n",
    "                            lines.append(line_content)\n",
    "\n",
    "                        code_content = \"\\n\".join(lines)\n",
    "                        yield f\"```{language}\\n{code_content}\\n```\\n\\n\"\n",
    "                    else:\n",
    "                        yield f\"`{child.get_text(strip=False)}`\"\n",
    "\n",
    "                elif child.name == \"p\":\n",
    "                    yield from get_text(child)\n",
    "                    yield \"\\n\\n\"\n",
    "                elif child.name == \"ul\":\n",
    "                    for li in child.find_all(\"li\", recursive=False):\n",
    "                        yield \"- \"\n",
    "                        yield from get_text(li)\n",
    "                        yield \"\\n\\n\"\n",
    "                elif child.name == \"ol\":\n",
    "                    for i, li in enumerate(child.find_all(\"li\", recursive=False)):\n",
    "                        yield f\"{i + 1}. \"\n",
    "                        yield from get_text(li)\n",
    "                        yield \"\\n\\n\"\n",
    "                elif child.name == \"div\" and \"tabs-container\" in child.attrs.get(\n",
    "                    \"class\", [\"\"]\n",
    "                ):\n",
    "                    tabs = child.find_all(\"li\", {\"role\": \"tab\"})\n",
    "                    tab_panels = child.find_all(\"div\", {\"role\": \"tabpanel\"})\n",
    "                    for tab, tab_panel in zip(tabs, tab_panels):\n",
    "                        tab_name = tab.get_text(strip=True)\n",
    "                        yield f\"{tab_name}\\n\"\n",
    "                        yield from get_text(tab_panel)\n",
    "                elif child.name == \"table\":\n",
    "                    thead = child.find(\"thead\")\n",
    "                    header_exists = isinstance(thead, Tag)\n",
    "                    if header_exists:\n",
    "                        headers = thead.find_all(\"th\")\n",
    "                        if headers:\n",
    "                            yield \"| \"\n",
    "                            yield \" | \".join(header.get_text() for header in headers)\n",
    "                            yield \" |\\n\"\n",
    "                            yield \"| \"\n",
    "                            yield \" | \".join(\"----\" for _ in headers)\n",
    "                            yield \" |\\n\"\n",
    "\n",
    "                    tbody = child.find(\"tbody\")\n",
    "                    tbody_exists = isinstance(tbody, Tag)\n",
    "                    if tbody_exists:\n",
    "                        for row in tbody.find_all(\"tr\"):\n",
    "                            yield \"| \"\n",
    "                            yield \" | \".join(\n",
    "                                cell.get_text(strip=True) for cell in row.find_all(\"td\")\n",
    "                            )\n",
    "                            yield \" |\\n\"\n",
    "\n",
    "                    yield \"\\n\\n\"\n",
    "                elif child.name in [\"button\"]:\n",
    "                    continue\n",
    "                else:\n",
    "                    yield from get_text(child)\n",
    "\n",
    "    joined = \"\".join(get_text(soup))\n",
    "    return re.sub(r\"\\n\\n+\", \"\\n\\n\", joined).strip()\n",
    "\n",
    "\n",
    "def metadata_extractor(\n",
    "    meta: dict, soup: BeautifulSoup, title_suffix: Optional[str] = None\n",
    ") -> dict:\n",
    "    title_element = soup.find(\"title\")\n",
    "    description_element = soup.find(\"meta\", attrs={\"name\": \"description\"})\n",
    "    html_element = soup.find(\"html\")\n",
    "    title = title_element.get_text() if title_element else \"\"\n",
    "    if title_suffix is not None:\n",
    "        title += title_suffix\n",
    "\n",
    "    return {\n",
    "        \"source\": meta[\"loc\"],\n",
    "        \"title\": title,\n",
    "        \"description\": description_element.get(\"content\", \"\")\n",
    "        if description_element\n",
    "        else \"\",\n",
    "        \"language\": html_element.get(\"lang\", \"\") if html_element else \"\",\n",
    "        **meta,\n",
    "    }\n",
    "\n",
    "def simple_extractor(html: str | BeautifulSoup) -> str:\n",
    "    if isinstance(html, str):\n",
    "        soup = BeautifulSoup(html, \"lxml\")\n",
    "    elif isinstance(html, BeautifulSoup):\n",
    "        soup = html\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            \"Input should be either BeautifulSoup object or an HTML string\"\n",
    "        )\n",
    "    return re.sub(r\"\\n\\n+\", \"\\n\\n\", soup.text).strip()\n",
    "\n",
    "def load_gigachat_docs():\n",
    "    return SitemapLoader(\n",
    "        \"https://developers.sber.ru/docs/sitemap.xml\",\n",
    "        filter_urls=[r\".*/gigachat/.*\"],\n",
    "        parsing_function=simple_extractor,\n",
    "        default_parser=\"lxml\",\n",
    "        bs_kwargs={\"parse_only\": SoupStrainer(name=(\"article\", \"title\"))},\n",
    "        meta_function=lambda meta, soup: metadata_extractor(\n",
    "            meta, soup, title_suffix=\" | GigaChat\"\n",
    "        ),\n",
    "    ).load()\n",
    "\n",
    "\n",
    "def load_langchain_gigachat_docs():\n",
    "    return SitemapLoader(\n",
    "        \"https://developers.sber.ru/docs/sitemap.xml\",\n",
    "        filter_urls=[r\".*/gigachain/.*\"],\n",
    "        parsing_function=simple_extractor,\n",
    "        default_parser=\"lxml\",\n",
    "        bs_kwargs={\"parse_only\": SoupStrainer(name=(\"article\", \"title\"))},\n",
    "        meta_function=lambda meta, soup: metadata_extractor(\n",
    "            meta, soup, title_suffix=\" | langchain-gigachat\"\n",
    "        ),\n",
    "    ).load()\n",
    "    \n",
    "def load_langchain_docs():\n",
    "    return SitemapLoader(\n",
    "        \"https://python.langchain.com/sitemap.xml\",\n",
    "        filter_urls=[\"https://python.langchain.com/\"],\n",
    "        parsing_function=langchain_docs_extractor,\n",
    "        default_parser=\"lxml\",\n",
    "        bs_kwargs={\n",
    "            \"parse_only\": SoupStrainer(\n",
    "                name=(\"article\", \"title\", \"html\", \"lang\", \"content\")\n",
    "            ),\n",
    "        },\n",
    "        meta_function=metadata_extractor,\n",
    "    ).load()\n",
    "\n",
    "def load_langgraph_docs():\n",
    "    return SitemapLoader(\n",
    "        \"https://langchain-ai.github.io/langgraph/sitemap.xml\",\n",
    "        parsing_function=simple_extractor,\n",
    "        default_parser=\"lxml\",\n",
    "        bs_kwargs={\"parse_only\": SoupStrainer(name=(\"article\", \"title\"))},\n",
    "        meta_function=lambda meta, soup: metadata_extractor(\n",
    "            meta, soup, title_suffix=\" | 🦜🕸️LangGraph\"\n",
    "        ),\n",
    "    ).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching pages: 100%|##########| 58/58 [00:14<00:00,  4.07it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "58"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gigachat_docs = load_gigachat_docs()\n",
    "len(gigachat_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching pages: 100%|##########| 22/22 [00:05<00:00,  4.34it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gigachain_docs = load_langchain_gigachat_docs()\n",
    "len(gigachain_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching pages: 100%|##########| 173/173 [00:32<00:00,  5.25it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "173"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "langggraph_docs = load_langgraph_docs()\n",
    "len(langggraph_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching pages: 100%|##########| 1421/1421 [03:39<00:00,  6.48it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1421"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "langchain_docs = load_langchain_docs()\n",
    "len(langchain_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1674"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_docs = []\n",
    "all_docs.extend(gigachat_docs)\n",
    "all_docs.extend(gigachain_docs)\n",
    "all_docs.extend(langggraph_docs)\n",
    "all_docs.extend(langchain_docs)\n",
    "len(all_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents splited. Count: 24279\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import MarkdownTextSplitter\n",
    "\n",
    "text_splitter = MarkdownTextSplitter(chunk_size=2000, chunk_overlap=500)\n",
    "doc_splits = text_splitter.split_documents(all_docs)\n",
    "print(f\"Documents splited. Count: {len(doc_splits)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tqdm\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "\n",
    "pinecone_api_key = os.environ.get(\"PINECONE_API_KEY\")\n",
    "pc = Pinecone(api_key=pinecone_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "index_name = os.environ.get(\"PINECONE_INDEX_NAME\", \"gigachain-test-gigar-newdb-giant\")\n",
    "pc.create_index(\n",
    "    name=index_name,\n",
    "    dimension=3072,\n",
    "    metric=\"cosine\",\n",
    "    spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\"),\n",
    ")\n",
    "while not pc.describe_index(index_name).status[\"ready\"]:\n",
    "    time.sleep(1)\n",
    "    \n",
    "index = pc.Index(index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_community.embeddings.gigachat import GigaChatEmbeddings\n",
    "# embeddings = GigaChatEmbeddings(model=\"EmbeddingsGigaR\")\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_pinecone import PineconeVectorStore\n",
    "vector_store = PineconeVectorStore(index=index, embedding=embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Загрузка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n"
     ]
    }
   ],
   "source": [
    "from uuid import uuid4\n",
    "uuids = [str(uuid4()) for _ in range(len(doc_splits))]\n",
    "vector_store.add_documents(documents=doc_splits, ids=uuids)\n",
    "retriever = vector_store.as_retriever()\n",
    "print(\"OK\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
